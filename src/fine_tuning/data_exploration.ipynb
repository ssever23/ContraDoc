{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Load NLI dataset into dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/ssever/ContraDoc/data/csv_files/nli_data_set.csv')\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\n",
    "    \"antonym\",\n",
    "    \"entailment\",\n",
    "    \"factive_antonym\",\n",
    "    \"factive_embedding_verb\",\n",
    "    \"lexical\",\n",
    "    \"negation\",\n",
    "    \"neutral\",\n",
    "    \"numeric\",\n",
    "    \"structure\",\n",
    "    \"temporal\",\n",
    "    \"worldknowledge\"\n",
    "]\n",
    "\n",
    "# Getting value counts\n",
    "value_counts = dataset['label_string'].value_counts()\n",
    "\n",
    "# Counting 'entailment' and 'neutral'\n",
    "label_counts = {label: value_counts.get(label, 0) for label in label_names}\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the numeric contradictions\n",
    "\n",
    "#filtered_df = dataset[dataset['label_string'] == 'numeric']\n",
    "#styled_df = filtered_df.style.set_properties(**{'text-align': 'left'})\n",
    "#styled_df.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Check for missing and duplicate values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = dataset.isnull().sum() # we get the number of missing data points per column\n",
    "print(\"Number of missing data points per column:\\n\")\n",
    "print (missing_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"is_duplicate\"] = dataset.duplicated()\n",
    "dataset[dataset[\"is_duplicate\"]==True].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Distribution of class labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = dataset['label_string'].value_counts()\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=181, counterclock=False)\n",
    "plt.title(\"NLI Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Split the Dataset into Train and Test and oversample Train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[['premise', 'hypothesis', 'label_string']]  # Features\n",
    "y = dataset['label']  # Labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize the RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "smote = SMOTE()\n",
    "\n",
    "# Applying oversampling only to the training set\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the change in size\n",
    "print(f\"Original train set size: {X.shape[0]}\")\n",
    "print(f\"Resampled train set size: {X_train_resampled.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train_resampled, y_train_resampled], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of rows and columns after split\n",
    "print(\"Train data: {} \\n\".format(train.shape))\n",
    "print(\"Test data: {} \\n\".format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Label distribution of train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_train = train['label_string'].value_counts()\n",
    "counts_test = test['label_string'].value_counts()\n",
    "\n",
    "fig, axis = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Train set plot\n",
    "axis[0].pie(counts_train, labels=counts_train.index, autopct='%1.1f%%', startangle=181, counterclock=False,)\n",
    "axis[0].set_title(\"Train set\")\n",
    "axis[0].axis('equal')\n",
    "\n",
    "# Test set plot\n",
    "axis[1].pie(counts_test, labels=counts_test.index, autopct='%1.1f%%', startangle=181, counterclock=False,)\n",
    "axis[1].set_title(\"Val set\")\n",
    "axis[1].axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('roberta-large')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Encode train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join premise with hypothesis\n",
    "#features_train = list(zip(train['premise'], train['hypothesis']))\n",
    "#features_test = list(zip(test['premise'], test['hypothesis']))\n",
    "features_train = train[['premise', 'hypothesis']].values.tolist()\n",
    "features_test = test[['premise', 'hypothesis']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text feature\n",
    "tokenized_features_train = tokenizer.batch_encode_plus(features_train)\n",
    "tokenized_features_test = tokenizer.batch_encode_plus(features_test)\n",
    "len(tokenized_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "train_resampled, test_resampled = ros.fit_resample(tokenized_features_train, tokenized_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_features_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenized data exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how XLM-RoBERTa tokenizes sentences\n",
    "print({x : tokenizer.encode(x, add_special_tokens=False) for x in dataset.values[0][1].split()})\n",
    "len(tokenizer.encode(dataset.values[0][1], add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect tokenized train features sentence length \n",
    "token_train_length = [len(x) for x in tokenized_features_train['input_ids']]\n",
    "print('max: ', max(token_train_length))\n",
    "print('min: ', min(token_train_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect tokenized test features sentence length \n",
    "token_test_length = [len(x) for x in tokenized_features_test['input_ids']]\n",
    "print('max: ', max(token_test_length))\n",
    "print('min: ', min(token_test_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot token train and test distribution\n",
    "\n",
    "fig, axis = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Train token plot\n",
    "axis[0].hist(token_train_length, rwidth = 0.9)\n",
    "axis[0].set_title(\"Train token distribution\", fontsize = 12)\n",
    "axis[0].set_xlabel(\"Token Length\")\n",
    "axis[0].set_ylabel(\"Frequency\") \n",
    "\n",
    "# Test token plot\n",
    "axis[1].hist(token_test_length, rwidth = 0.9)\n",
    "axis[1].set_title(\"Test token distribution\", fontsize = 12)\n",
    "axis[1].set_xlabel(\"Token Length\")\n",
    "axis[1].set_ylabel(\"Frequency\") \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Box plot of embedding similarity scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "#model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and embed premises and hypotheses\n",
    "\n",
    "data = pd.read_csv('/home/ssever/ContraDoc/data/csv_files/nli_data_set.csv')\n",
    "datalist = list(zip(data['premise'], data['hypothesis'], data['label_string']))\n",
    "\n",
    "encoded_list = [(model.encode(premise), model.encode(hypothesis), label) for premise, hypothesis, label in datalist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity scores\n",
    "\n",
    "scores_list = [(util.cos_sim(premise, hypothesis), label) for premise, hypothesis, label in encoded_list]\n",
    "scores_list = [(round(float(tensor[0][0]), 5), label) for tensor, label in scores_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target labels\n",
    "target_names = [\n",
    "    \"antonym\",\n",
    "    \"entailment\",\n",
    "    \"factive_antonym\",\n",
    "    \"factive_embedding_verb\",\n",
    "    \"lexical\",\n",
    "    \"negation\",\n",
    "    \"neutral\",\n",
    "    \"numeric\",\n",
    "    \"structure\",\n",
    "    \"temporal\",\n",
    "    \"worldknowledge\"\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(scores_list, columns=['Score', 'Label'])\n",
    "df = df[df['Label'].isin(target_names)]\n",
    "\n",
    "# Create a box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Label', y='Score', data=df, order=target_names, medianprops={'color': 'white'})\n",
    "\n",
    "# Plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.title('Box plot of similarity scores by label')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Similarity Scores')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking outliers for entailment pairs\n",
    "\n",
    "zipped = list(zip(data['premise'], data['hypothesis'], scores_list))\n",
    "entailments = [element for element in zipped if element[2][1] == 'entailment' and element[2][0] <= 0.4]\n",
    "len(entailments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Backup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **Backup**\n",
    "# Calculate means and draw a line for each box at the mean position\n",
    "for i, label in enumerate(target_names):\n",
    "    label_scores = df[df['Label'] == label]['Score']\n",
    "    mean_score = label_scores.mean()\n",
    "    \n",
    "    # Draws a horizontal line for the mean\n",
    "    plt.hlines(mean_score, i - 0.4, i + 0.4, colors='white', linestyles='dashed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
