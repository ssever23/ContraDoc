{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface libraries \n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, XLMRobertaConfig\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import TFAutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset, TensorDataset\n",
    "\n",
    "# additional libraries\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "\n",
    "#viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the logging level to error to suppress warnings (and info messages)\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface authetication for model download\n",
    "\n",
    "#from huggingface_hub import login\n",
    "#login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Initiate cuda**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device type:', device)\n",
    "print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "print('GPU is:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Load NLI dataset into dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/ssever/ContraDoc/data/csv_files/nli_data_set.csv')\n",
    "\n",
    "#dataset = dataset[dataset.label_string != 'worldknowledge']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transform dataset to 3 labels (Use only for 3-label classification!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_map = {\n",
    "    \"antonym\": \"contradiction\",\n",
    "    \"factive_antonym\": \"contradiction\",\n",
    "    \"factive_embedding_verb\": \"contradiction\",\n",
    "    \"lexical\": \"contradiction\",\n",
    "    \"negation\": \"contradiction\",\n",
    "    \"numeric\": \"contradiction\",\n",
    "    \"structure\": \"contradiction\",\n",
    "    \"temporal\": \"contradiction\",\n",
    "    \"worldknowledge\": \"contradiction\"\n",
    "}\n",
    "\n",
    "# Replace all labels except 'neutral' and 'entailment' with 'contradiction'\n",
    "dataset['label_string'] = dataset['label_string'].replace(replacement_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify unique words and sort them to maintain consistency\n",
    "unique_words = sorted(dataset['label_string'].unique())\n",
    "\n",
    "# Create a mapping from words to digits\n",
    "word_to_digit = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "\n",
    "# Apply the mapping to the 'label' column\n",
    "dataset['label'] = dataset['label_string'].map(word_to_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create holdout test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, test = train_test_split(dataset, stratify=dataset.label.values, \n",
    "                                                  random_state=42, # note that holdout test set distribution is permanently set\n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# check the number of rows and columns after split\n",
    "print(\"Data size: {} \\n\".format(data.shape))\n",
    "print(\"Test size: {}\".format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create train and validation data sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(data, stratify=data.label.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# check the number of rows and columns after split\n",
    "print(\"Train size: {} \\n\".format(train.shape))\n",
    "print(\"Validation size: {}\".format(val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Check number of classes again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for number of unique labels\n",
    "dataset['label_string'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed (function called down under training)\n",
    "def seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_val)\n",
    "    else:\n",
    "        torch.manual_seed(seed_val)\n",
    "\n",
    "# Choose a model\n",
    "#model_name ='bert-base-cased'\n",
    "#model_name ='bert-large-cased'\n",
    "model_name ='roberta-base'\n",
    "#model_name ='roberta-large'\n",
    "#model_name ='xlm-roberta-base'\n",
    "#model_name = 'xlm-roberta-large'\n",
    "#model_name = 'symanto/xlm-roberta-base-snli-mnli-anli-xnli'\n",
    "#model_name = 'joeddav/xlm-roberta-large-xnli'\n",
    "\n",
    "# Initialize tokenizer\n",
    "if model_name == 'joeddav/xlm-roberta-large-xnli':\n",
    "    tokenizer = AutoTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli', use_fast=False)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Function for encoding input data\n",
    "def encode_sets(data, tokenizer):\n",
    "    kwargs = { 'truncation': True,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'padding': 'max_length',\n",
    "    'return_attention_mask': True, \n",
    "    'return_token_type_ids': True     \n",
    "    }\n",
    "    datalist = list(zip(data['premise'], data['hypothesis']))\n",
    "    tokenized = tokenizer.batch_encode_plus(datalist,**kwargs)\n",
    "    input_ids = torch.LongTensor(tokenized.input_ids)\n",
    "    attention_masks = torch.LongTensor(tokenized.attention_mask)\n",
    "    token_type_ids = torch.LongTensor(tokenized.token_type_ids)\n",
    "    return input_ids, attention_masks, token_type_ids\n",
    "\n",
    "# Get number of labels in dataset\n",
    "num_classes = dataset['label'].nunique()\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    label_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    label_names = [\n",
    "        \"antonym\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonym\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "# Create class weights for weighted loss\n",
    "class_counts = []\n",
    "\n",
    "for name in label_names:\n",
    "    class_counts.append(val['label_string'].value_counts()[name])\n",
    "\n",
    "total_counts = sum(class_counts)\n",
    "weights = [total_counts / class_count for class_count in class_counts]\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Set cross-entropy loss as loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "epochs_list = []\n",
    "\n",
    "# Set arrays for loss and prediction results\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "test_loss = []\n",
    "\n",
    "val_preds = []\n",
    "true_classes = []\n",
    "accuracies = []\n",
    "\n",
    "test_preds = []\n",
    "test_true_classes = []\n",
    "test_accuracies = []\n",
    "\n",
    "learning_rate = []\n",
    "batch_size = []\n",
    "\n",
    "# Number of searches\n",
    "n_searches = 1\n",
    "\n",
    "#BATCH_SIZE = [16]\n",
    "#L_RATE = [1e-6, 2e-6, 3e-6, 4e-6, 5e-6, 6e-6, 7e-6, 8e-6, 9e-6]\n",
    "\n",
    "for search in range(n_searches):\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    seed(1024)\n",
    "\n",
    "    train_loss_search = []\n",
    "    val_loss_search = []\n",
    "    test_loss_search = []\n",
    "\n",
    "    val_preds_search = []\n",
    "    true_classes_search = []\n",
    "    accuracies_search = []\n",
    "\n",
    "    test_preds_search = []\n",
    "    test_true_classes_search = []\n",
    "    test_accuracies_search = []\n",
    "\n",
    "    # Hyperparameter settings\n",
    "    #BATCH_SIZE = random.choice([8, 16, 32, 64])\n",
    "    BATCH_SIZE = 16\n",
    "    #L_RATE = 10 ** random.uniform(-5, -6)\n",
    "    L_RATE = 5.5e-06\n",
    "    #L_RATE = 1e-05\n",
    "    MAX_LENGTH = 256\n",
    "    NUM_EPOCHS = 80\n",
    "\n",
    "    # Encode train set\n",
    "    input_ids, attention_masks, token_type_ids = encode_sets(train, tokenizer)\n",
    "    labels = torch.Tensor(train['label']).reshape(-1, 1)\n",
    "    train_tensor = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "    train_dataloader = DataLoader(train_tensor, sampler=RandomSampler(train_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Encode validation set\n",
    "    input_ids, attention_masks, token_type_ids = encode_sets(val,tokenizer)\n",
    "    labels = torch.Tensor(val['label']).reshape(-1, 1)\n",
    "    val_tensor = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "    val_dataloader = DataLoader(val_tensor, sampler=SequentialSampler(val_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Early stopping settings\n",
    "    patience = 10\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    #stop_epochs = 0\n",
    "\n",
    "    # Initialize model\n",
    "    if model_name == \"bert-base-cased\" or model_name == 'bert-large-cased':\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes, output_hidden_states=False, output_attentions=False, problem_type=\"multi_label_classification\")\n",
    "    elif model_name == \"roberta-base\" or model_name == 'roberta-large':\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "    elif model_name == \"xlm-roberta-base\" or model_name == 'xlm-roberta-large' or ((model_name == 'symanto/xlm-roberta-base-snli-mnli-anli-xnli' or model_name == 'joeddav/xlm-roberta-large-xnli') and num_classes == 3):\n",
    "        model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "    elif (model_name == 'symanto/xlm-roberta-base-snli-mnli-anli-xnli' or model_name == 'joeddav/xlm-roberta-large-xnli') and num_classes == 11:\n",
    "        config = XLMRobertaConfig.from_pretrained(model_name, num_labels=num_classes)\n",
    "        model = XLMRobertaForSequenceClassification(config)\n",
    "        \n",
    "    \"\"\"\n",
    "    # Define LoRA configuration\n",
    "    r = 16\n",
    "    lora_alpha = r*2\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=r,            # Rank of the low-rank matrices\n",
    "    lora_alpha=lora_alpha,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout rate\n",
    "    target_modules=[\"query\", \"key\", \"value\"]  # Apply LoRA only to attention matrices\n",
    ")\n",
    "\n",
    "    # Apply LoRA configuration to model\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move model to device (GPU)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Print model and training details\n",
    "\n",
    "    print(f\"\\nNumber of classes: {num_classes}\")\n",
    "    print(f\"\\nModel: {model_name}\\n\")\n",
    "    #model.print_trainable_parameters()\n",
    "    print(f\"\\nLearning Rate = {L_RATE} Batch Size = {BATCH_SIZE}\\n\")\n",
    "    print(f\"Number of batches in train set: {len(train_dataloader)}\")\n",
    "    print(f\"Number of batches in validation set: {len(val_dataloader)}\\n\")\n",
    "\n",
    "    # Load optimizer\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                lr = L_RATE, \n",
    "                eps = 1e-8\n",
    "                )\n",
    "    \n",
    "    \"\"\" # Create scheduler\n",
    "    total_steps = len(train_dataloader) * NUM_EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps = 0.1 * len(train_dataloader),\n",
    "                                                num_training_steps = total_steps)\"\"\"\n",
    "\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        # Training:\n",
    "\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        total_train_loss=0\n",
    "            \n",
    "        for i,batch in tqdm(enumerate(train_dataloader)):\n",
    "            # clear previous gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids, attention_masks, token_type_ids = (batch[0].to(device), \n",
    "                                                          batch[1].to(device),\n",
    "                                                          batch[2].to(device))\n",
    "            \n",
    "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "\n",
    "            #loss=outputs[0]\n",
    "            labels = torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device)\n",
    "            #labels = torch.argmax(batch[3], dim=1).to(device)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Backpropagation:\n",
    "            \n",
    "            # calculate gradients\n",
    "            loss.backward()\n",
    "            # clip the norm of the gradients to 1.0. (prevents gradient explosion)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # parameter update\n",
    "            optimizer.step()\n",
    "            \n",
    "            # scheduler update\n",
    "            #scheduler.step()\n",
    "\n",
    "        # Validation:\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        #torch.set_grad_enabled(False)\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for j, batch in enumerate(val_dataloader):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                input_ids, attention_masks, token_type_ids = (batch[0].to(device), \n",
    "                                                              batch[1].to(device),\n",
    "                                                              batch[2].to(device))\n",
    "                \n",
    "                start_time = time.time()\n",
    "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "                end_time = time.time()\n",
    "\n",
    "            #loss = outputs[0]\n",
    "            logits = outputs.logits\n",
    "            labels = torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device)\n",
    "            #labels = torch.argmax(batch[3], dim=1).to(device)\n",
    "            loss = criterion(logits, labels)\n",
    "            true = batch[3]\n",
    "\n",
    "            model_preds = logits.detach().cpu().numpy()\n",
    "            total_val_loss += loss.item()\n",
    "            #stop_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "            if j == 0:  # first batch\n",
    "                stacked_model_preds = model_preds\n",
    "                true_labels = true\n",
    "            else:\n",
    "                stacked_model_preds = np.vstack((stacked_model_preds, model_preds))\n",
    "                true_labels = np.vstack((true_labels, true))\n",
    "    \n",
    "        # Train loss results per epoch\n",
    "        #print(f'Training loss on epoch {epoch + 1}: {total_train_loss}')\n",
    "        train_loss_search.append(total_train_loss)\n",
    "\n",
    "        # Validation loss results per epoch\n",
    "        #print(f'\\n------------------------------------\\nValidation loss on epoch {epoch + 1}: {total_val_loss}\\n------------------------------------\\n')\n",
    "\n",
    "        val_preds_search.append(np.argmax(stacked_model_preds, axis=1))\n",
    "        true_classes_search.append(true_labels.flatten().astype('int64'))\n",
    "        accuracies_search.append(np.sum(val_preds_search[epoch] == true_classes_search[epoch])/len(val_preds_search[epoch]))\n",
    "        val_loss_search.append(total_val_loss)\n",
    "        \"\"\"\n",
    "        # Testing:\n",
    "\n",
    "        # Encode test set\n",
    "        input_ids, attention_masks, token_type_ids = encode_sets(test,tokenizer)\n",
    "        labels = torch.Tensor(test['label']).reshape(-1, 1)\n",
    "        test_tensor = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "        test_dataloader = DataLoader(test_tensor, sampler=SequentialSampler(test_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_test_loss = 0\n",
    "\n",
    "        for j, batch in enumerate(test_dataloader):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                input_ids, attention_masks, token_type_ids = (batch[0].to(device), \n",
    "                                                                batch[1].to(device),\n",
    "                                                                batch[2].to(device))\n",
    "                \n",
    "                start_time = time.time()\n",
    "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "                end_time = time.time()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            # Predicted labels\n",
    "            labels = torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device)\n",
    "            loss = criterion(logits, labels)\n",
    "            # True labels\n",
    "            test_true = batch[3]\n",
    "\n",
    "            model_preds = logits.detach().cpu().numpy()\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            if j == 0:  # first batch\n",
    "                stacked_test_preds = model_preds\n",
    "                true_labels = test_true\n",
    "            else:\n",
    "                stacked_test_preds = np.vstack((stacked_test_preds, model_preds))\n",
    "                true_labels = np.vstack((true_labels, test_true))\n",
    "\n",
    "        test_preds_search.append(np.argmax(stacked_test_preds, axis=1))\n",
    "        test_true_classes_search.append(true_labels.flatten().astype('int64'))\n",
    "        test_accuracies_search.append(np.sum(test_preds_search[epoch] == test_true_classes_search[epoch])/len(test_preds_search[epoch]))\n",
    "        test_loss_search.append(total_test_loss)\n",
    "        \"\"\"\n",
    "        # Early stopping\n",
    "        if total_val_loss < best_loss:\n",
    "            best_loss = total_val_loss\n",
    "            patience_counter = 0\n",
    "            # Saving model\n",
    "            #torch.save(model, f\"roberta-base.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter == patience:\n",
    "            # Calculate inference time\n",
    "            inference_time = end_time - start_time\n",
    "            print(f\"\\nInference time: {inference_time:.4f} seconds\\n\")\n",
    "\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
    "            break\n",
    "        \n",
    "    total_end_time = time.time()\n",
    "    print(f\"\\nTotal time for training: {(total_end_time - total_start_time)/60:.2f} minutes\")\n",
    "    \n",
    "\n",
    "    learning_rate.append(L_RATE)\n",
    "    batch_size.append(BATCH_SIZE)\n",
    "    epochs_list.append(epoch)\n",
    "    train_loss.append(train_loss_search)\n",
    "    val_loss.append(val_loss_search)\n",
    "    test_loss.append(test_loss_search)\n",
    "\n",
    "    val_preds.append(val_preds_search)\n",
    "    true_classes.append(true_classes_search)\n",
    "    accuracies.append(accuracies_search)\n",
    "    \"\"\"\n",
    "    test_preds.append(test_preds_search)\n",
    "    test_true_classes.append(test_true_classes_search)\n",
    "    test_accuracies.append(test_accuracies_search)\n",
    "    \"\"\"\n",
    "    # Clear memory\n",
    "    del model\n",
    "    del optimizer\n",
    "    #del scheduler\n",
    "    del train_tensor\n",
    "    del val_tensor\n",
    "    del train_dataloader\n",
    "    del val_dataloader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loss graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for e in range(0, len(epochs_list)):\n",
    "\n",
    "    epochs = list(range(1, epochs_list[e]+2))\n",
    "\n",
    "    # Normalize losses for visualization\n",
    "    max_train_loss = max(train_loss[e])\n",
    "    training_loss_normalized = [loss / max_train_loss for loss in train_loss[e]]\n",
    "    max_val_loss = max(val_loss[e])\n",
    "    val_loss_normalized = [loss / max_val_loss for loss in val_loss[e]]\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(epochs, training_loss_normalized, label='train loss', color='blue')\n",
    "    plt.plot(epochs, val_loss_normalized, label='val loss', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xticks(np.linspace(0, 25, 6, endpoint=True))\n",
    "    plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    # Format x-axis tick labels to display as integers\n",
    "    from matplotlib.ticker import FormatStrFormatter\n",
    "    plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "    plt.ylabel('Normalized Loss')\n",
    "    #plt.ylim(0.9, 1.0)\n",
    "    #plt.title(f'Search: {e+1} LR: {learning_rate[e]} BS: {batch_size[e]}')\n",
    "    plt.title(model_name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results of epoch with smallest test loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smallest test loss\n",
    "\n",
    "index_list = []\n",
    "\n",
    "for v in range(len(val_loss)):\n",
    "    lowest = min(val_loss[v])\n",
    "    index_lowest = val_loss[v].index(lowest)\n",
    "    acc = accuracies[v][index_lowest]\n",
    "    index_list.append(index_lowest)\n",
    "    acc_max = accuracies[v].index(max(accuracies[v]))\n",
    "    print(f\"Search {v+1} | Total epochs: {epochs_list[v]+1} | Best epoch: {index_lowest + 1} with loss: {lowest} and accuracy of {round(acc,4)*100}% | LR: {learning_rate[v]} | BS: {batch_size[v]}\")\n",
    "    print(f\"Highest accuracy: {round(max(accuracies[v]),4)*100}% at epoch {acc_max + 1}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Classification reports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification reports\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    target_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    target_names = [\n",
    "        \"antonym\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonym\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "classification_reports = []\n",
    "\n",
    "filename= \"cr_reports.txt\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    for e in range(len(epochs_list)):\n",
    "        for epoch in range(epochs_list[e]):\n",
    "            cr = classification_report(true_classes[e][epoch], \n",
    "                                val_preds[e][epoch],\n",
    "                                target_names=target_names, \n",
    "                                digits=4)\n",
    "            classification_reports.append(cr)\n",
    "\n",
    "            file.write(f\"Report of Search {e+1} for Epoch {epoch + 1}:\\n\\n{cr}\\n\")\n",
    "            file.write(\"-----End of Report-----\\n\\n\")\n",
    "        file.write(\"-----End of Search-----\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification report of epoch with lowest loss\n",
    "\n",
    "filename= \"cr_reports.txt\"\n",
    "\n",
    "with open(filename, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "searches = content.split(\"-----End of Search-----\\n\\n\")\n",
    "\n",
    "searches = [search for search in searches if search]\n",
    "\n",
    "reports = []\n",
    "\n",
    "for search in searches:\n",
    "    report = search.split(\"-----End of Report-----\\n\\n\")\n",
    "    reports.append(report)\n",
    "\n",
    "reports = [report for report in reports if report]\n",
    "\n",
    "i = 0\n",
    "step = 0\n",
    "\n",
    "for report in reports:\n",
    "    if step != 0:\n",
    "        i += 1\n",
    "    for r in report:\n",
    "        if report.index(r) == index_list[i]:\n",
    "            print(r)\n",
    "            step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    target_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    target_names = [\n",
    "        \"antonym\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonym\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "cm = confusion_matrix(true_classes[0][index_lowest], val_preds[0][index_lowest])\n",
    "\n",
    "# Plotting the confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(f'Confusion Matrix Val')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test_accuracies[0][12]\n",
    "print(f\"accuracy of {round(acc,4)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smallest test loss\n",
    "\n",
    "index_list = []\n",
    "\n",
    "for v in range(len(val_loss)):\n",
    "    #lowest = min(val_loss[v])\n",
    "    #index_lowest = test_loss[v][13]\n",
    "    acc = test_accuracies[v][12]\n",
    "    print(f\"accuracy of {round(acc,4)*100}%\")\n",
    "    #index_list.append(index_lowest)\n",
    "    #acc_max = test_accuracies[v].index(max(test_accuracies[v]))\n",
    "    #print(f\"Search {v+1} | Total epochs: {epochs_list[v]+1} | Best epoch: {index_lowest + 1} with loss: {lowest} and accuracy of {round(acc,4)*100}% | LR: {learning_rate[v]} | BS: {batch_size[v]}\")\n",
    "    #print(f\"Highest accuracy: {round(max(test_accuracies[v]),4)*100}% at epoch {acc_max + 1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification reports\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    target_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    target_names = [\n",
    "        \"antonym\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonym\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "classification_reports = []\n",
    "\n",
    "filename= \"test_reports.txt\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    for e in range(len(epochs_list)):\n",
    "        for epoch in range(epochs_list[e]):\n",
    "            cr = classification_report(test_true_classes[e][epoch], \n",
    "                                test_preds[e][epoch],\n",
    "                                target_names=target_names, \n",
    "                                digits=4)\n",
    "            classification_reports.append(cr)\n",
    "\n",
    "            file.write(f\"Report of Search {e+1} for Epoch {epoch + 1}:\\n\\n{cr}\\n\")\n",
    "            file.write(\"-----End of Report-----\\n\\n\")\n",
    "        file.write(\"-----End of Search-----\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    target_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    target_names = [\n",
    "        \"antonym\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonym\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "cm = confusion_matrix(test_true_classes[0][12], test_preds[0][12])\n",
    "\n",
    "# Plotting the confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(f'Confusion Matrix Test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Appendix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Test contradiction pre-trained models directly without fine-tuning on NLI dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli', use_fast=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained('symanto/xlm-roberta-base-snli-mnli-anli-xnli')\n",
    "\n",
    "#model_name = 'joeddav/xlm-roberta-large-xnli'\n",
    "model_name = 'symanto/xlm-roberta-base-snli-mnli-anli-xnli'\n",
    "\n",
    "# Function for encoding input data\n",
    "def encode_sets(data, tokenizer):\n",
    "    kwargs = { 'truncation': True,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'padding': 'max_length',\n",
    "    'return_attention_mask': True, \n",
    "    'return_token_type_ids': True     \n",
    "    }\n",
    "    datalist = list(zip(data['premise'], data['hypothesis']))\n",
    "    tokenized = tokenizer.batch_encode_plus(datalist,**kwargs)\n",
    "    input_ids = torch.LongTensor(tokenized.input_ids)\n",
    "    attention_masks = torch.LongTensor(tokenized.attention_mask)\n",
    "    token_type_ids = torch.LongTensor(tokenized.token_type_ids)\n",
    "    return input_ids, attention_masks, token_type_ids\n",
    "\n",
    "# Get number of labels in dataset\n",
    "num_classes = dataset['label'].nunique()\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    label_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    label_names = [\n",
    "        \"antonym\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonym\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "    \n",
    "# Set cross-entropy loss as loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Testing:\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# Encode test set\n",
    "input_ids, attention_masks, token_type_ids = encode_sets(test,tokenizer)\n",
    "labels = torch.Tensor(test['label']).reshape(-1, 1)\n",
    "test_tensor = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "test_dataloader = DataLoader(test_tensor, sampler=SequentialSampler(test_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "\n",
    "for j, batch in enumerate(test_dataloader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_ids, attention_masks, token_type_ids = (batch[0].to(device), \n",
    "                                                        batch[1].to(device),\n",
    "                                                        batch[2].to(device))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "        end_time = time.time()\n",
    "\n",
    "    logits = outputs.logits\n",
    "    # Predicted labels\n",
    "    labels = torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device)\n",
    "    loss = criterion(logits, labels)\n",
    "    # True labels\n",
    "    test_true = batch[3]\n",
    "\n",
    "    test_preds = logits.detach().cpu().numpy()\n",
    "    test_loss += loss.item()\n",
    "\n",
    "    if j == 0:  # first batch\n",
    "        stacked_test_preds = test_preds\n",
    "        true_labels = test_true\n",
    "    else:\n",
    "        stacked_test_preds = np.vstack((stacked_test_preds, test_preds))\n",
    "        true_labels = np.vstack((true_labels, test_true))\n",
    "\n",
    "test_preds = np.argmax(stacked_test_preds, axis=1)\n",
    "test_true_classes = true_labels.flatten().astype('int64')\n",
    "test_accuracy = np.sum(test_preds == test_true_classes)/len(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = np.sum(test_preds == test_true_classes)/len(test_preds)\n",
    "print(f\"Accuracy on test set: {round(test_accuracy,3)*100}% with loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Unfreezing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_layers(model, last_n_layers):\n",
    "    # Reverse the model layers and unfreeze the last n layers\n",
    "    layers_to_unfreeze = list(model.roberta.encoder.layer.children())[-last_n_layers:]\n",
    "    for layer in layers_to_unfreeze:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "test_preds = []\n",
    "true_classes = []\n",
    "accuracies = []\n",
    "\n",
    "phases = [2, 4, 6, 8, 10, 12]\n",
    "for index, phase in enumerate(phases):\n",
    "    unfreeze_layers(model, phase)\n",
    "    print(f\"Training with last {phase} layers unfrozen...\")\n",
    "\n",
    "    train_loss_phase = []\n",
    "    test_loss_phase = []\n",
    "\n",
    "    test_preds_phase = []\n",
    "    true_classes_phase = []\n",
    "    accuracies_phase = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        total_train_loss=0\n",
    "            \n",
    "        for i,batch in tqdm(enumerate(train_dataloader)):\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids, attention_masks, token_type_ids, labels=(batch[0].to(device), \n",
    "                                                                batch[1].to(device),\n",
    "                                                                batch[2].to(device),  \n",
    "                                                                torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device))\n",
    "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks, labels=labels)\n",
    "            loss=outputs[0]\n",
    "            #if i%10==0:\n",
    "                #print(f'loss of batch {i}: {loss}')\n",
    "            total_train_loss+=loss.item()\n",
    "\n",
    "            # Backpropagation:\n",
    "            # calculate gradients\n",
    "            loss.backward()\n",
    "            # clip the norm of the gradients to 1.0. (prevents gradient explosion)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # parameter update\n",
    "            optimizer.step()\n",
    "\n",
    "            # Scheduler update\n",
    "            #scheduler.step()\n",
    "        \n",
    "        print(f'Training loss on epoch {epoch + 1}: {total_train_loss}')\n",
    "        train_loss_phase.append(total_train_loss)\n",
    "\n",
    "        # Testing\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        torch.set_grad_enabled(False)\n",
    "        total_test_loss = 0\n",
    "\n",
    "        for j, batch in enumerate(val_dataloader):\n",
    "            input_ids, attention_masks, token_type_ids, labels=(batch[0].to(device), \n",
    "                                                                batch[1].to(device),\n",
    "                                                                batch[2].to(device), \n",
    "                                                                torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device))\n",
    "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks, labels = labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            logits = outputs.logits\n",
    "            true = batch[3]\n",
    "\n",
    "            val_preds = logits.detach().cpu().numpy()\n",
    "            total_test_loss += loss.item()\n",
    "            if j == 0:  # first batch\n",
    "                stacked_val_preds = val_preds\n",
    "                true_labels = true\n",
    "\n",
    "            else:\n",
    "                stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n",
    "                true_labels = np.vstack((true_labels, true))\n",
    "                #stacked_val_preds.extend(val_preds)\n",
    "            #print(len(stacked_val_preds))\n",
    "        \n",
    "        print(f'\\n------------------------------------\\nTest loss on epoch {epoch + 1}: {total_test_loss}\\n------------------------------------\\n')\n",
    "\n",
    "        # Prediction results per epoch\n",
    "\n",
    "        test_preds_phase.append(np.argmax(stacked_val_preds, axis=1))\n",
    "        true_classes_phase.append(true_labels.flatten().astype('int64'))\n",
    "        accuracies_phase.append(np.sum(test_preds_phase[epoch] == true_classes_phase[epoch])/len(test_preds_phase[epoch]))\n",
    "\n",
    "        # Test loss per epoch\n",
    "\n",
    "        test_loss_phase.append(total_test_loss)\n",
    "    \n",
    "    train_loss.append(train_loss_phase)\n",
    "    test_loss.append(test_loss_phase)\n",
    "\n",
    "    test_preds.append(test_preds_phase)\n",
    "    true_classes.append(true_classes_phase)\n",
    "    accuracies.append(accuracies_phase)\n",
    "\n",
    "    NUM_EPOCHS = NUM_EPOCHS + 5\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\n",
    "    \"antonym\",\n",
    "    \"entailment\",\n",
    "    \"factive_antonym\",\n",
    "    \"factive_embedding_verb\",\n",
    "    \"lexical\",\n",
    "    \"negation\",\n",
    "    \"neutral\",\n",
    "    \"numeric\",\n",
    "    \"structure\",\n",
    "    \"temporal\",\n",
    "    \"worldknowledge\"\n",
    "]   \n",
    "\n",
    "classification_reports = []\n",
    "\n",
    "filename= \"test.txt\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    for epoch in range(stop_epochs-1):\n",
    "        cr = classification_report(true_classes[epoch], \n",
    "                            val_preds[epoch],\n",
    "                            target_names=target_names, \n",
    "                            digits=4)\n",
    "        classification_reports.append(cr)\n",
    "\n",
    "        file.write(f\"Report for Epoch {epoch + 1}:\\n{cr}\\n\")\n",
    "        file.write(\"-----End of Report-----\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= \"test.txt\"\n",
    "\n",
    "with open(filename, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Split the content back into individual reports using the delimiter\n",
    "# This creates a list where each element is a report\n",
    "reports = content.split(\"-----End of Report-----\\n\\n\")\n",
    "\n",
    "# Remove any empty strings in case they exist\n",
    "reports = [report for report in reports if report]\n",
    "\n",
    "# Iterate over the reports and print them\n",
    "for report in reports[index_lowest:index_lowest+1]:\n",
    "    print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
