{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "# huggingface libraries \n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, XLMRobertaConfig\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import TFAutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset, TensorDataset\n",
    "\n",
    "# additional libraries\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "\n",
    "#viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the logging level to error to suppress warnings (and info messages)\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface authetication for model download\n",
    "\n",
    "#from huggingface_hub import login\n",
    "#login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Initiate cuda**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: cuda\n",
      "There are 1 GPU(s) available.\n",
      "GPU is: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device type:', device)\n",
    "print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "print('GPU is:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Load NLI dataset into dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/ssever/contradiction-detection/data/csv_files/nli_data_set.csv')\n",
    "\n",
    "#dataset = dataset[dataset.label_string != 'worldknowledge']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transform dataset to 3 labels (Use only for 3-label classification!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_map = {\n",
    "    \"antonymity\": \"contradiction\",\n",
    "    \"factive_antonymity\": \"contradiction\",\n",
    "    \"factive_embedding_verb\": \"contradiction\",\n",
    "    \"lexical\": \"contradiction\",\n",
    "    \"negation\": \"contradiction\",\n",
    "    \"numeric\": \"contradiction\",\n",
    "    \"structure\": \"contradiction\",\n",
    "    \"temporal\": \"contradiction\",\n",
    "    \"worldknowledge\": \"contradiction\"\n",
    "}\n",
    "\n",
    "# Replace all labels except 'neutral' and 'entailment' with 'contradiction'\n",
    "dataset['label_string'] = dataset['label_string'].replace(replacement_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify unique words and sort them to maintain consistency\n",
    "unique_words = sorted(dataset['label_string'].unique())\n",
    "\n",
    "# Create a mapping from words to digits\n",
    "word_to_digit = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "\n",
    "# Apply the mapping to the 'label' column\n",
    "dataset['label'] = dataset['label_string'].map(word_to_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create holdout test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: (3097, 5) \n",
      "\n",
      "Test size: (345, 5)\n"
     ]
    }
   ],
   "source": [
    "data, test = train_test_split(dataset, stratify=dataset.label.values, \n",
    "                                                  random_state=42, # note that holdout test set distribution is permanently set\n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# check the number of rows and columns after split\n",
    "print(\"Data size: {} \\n\".format(data.shape))\n",
    "print(\"Test size: {}\".format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create train and validation data sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (2787, 5) \n",
      "\n",
      "Validation size: (310, 5)\n"
     ]
    }
   ],
   "source": [
    "train, val = train_test_split(data, stratify=data.label.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# check the number of rows and columns after split\n",
    "print(\"Train size: {} \\n\".format(train.shape))\n",
    "print(\"Validation size: {}\".format(val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Check number of classes again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for number of unique labels\n",
    "dataset['label_string'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Run untrained model on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False  # Disable auto-tuning\n",
    "\n",
    "#set_seed(42)\n",
    "\n",
    "model_name ='roberta-base'\n",
    "\n",
    "# Get number of labels in dataset\n",
    "num_classes = dataset['label'].nunique()\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Initialize model\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def encode_sets(data, tokenizer):\n",
    "    kwargs = { 'truncation': True,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'padding': 'max_length',\n",
    "    'return_attention_mask': True, \n",
    "    'return_token_type_ids': True     \n",
    "    }\n",
    "    datalist = list(zip(data['premise'], data['hypothesis']))\n",
    "    tokenized = tokenizer.batch_encode_plus(datalist,**kwargs)\n",
    "    input_ids = torch.LongTensor(tokenized.input_ids)\n",
    "    attention_masks = torch.LongTensor(tokenized.attention_mask)\n",
    "    token_type_ids = torch.LongTensor(tokenized.token_type_ids)\n",
    "    return input_ids, attention_masks, token_type_ids\n",
    "\n",
    "# Set hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# Encode validation set\n",
    "input_ids, attention_masks, token_type_ids = encode_sets(test,tokenizer)\n",
    "labels = torch.Tensor(test['label']).reshape(-1, 1)\n",
    "test_tensor = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "test_dataloader = DataLoader(test_tensor, sampler=SequentialSampler(test_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Testing:\n",
    "        \n",
    "model.eval()\n",
    "\n",
    "for j, batch in enumerate(test_dataloader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_ids, attention_masks, token_type_ids = (batch[0].to(device), \n",
    "                                                        batch[1].to(device),\n",
    "                                                        batch[2].to(device))\n",
    "        \n",
    "        #start_time = time.time()\n",
    "        outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "        #end_time = time.time()\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    #labels = torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device)\n",
    "    #labels = torch.argmax(batch[3], dim=1).to(device)\n",
    "    true = batch[3]\n",
    "\n",
    "    model_preds = logits.detach().cpu().numpy()\n",
    "\n",
    "    if j == 0:  # first batch\n",
    "        stacked_model_preds = model_preds\n",
    "        true_labels = true\n",
    "    else:\n",
    "        stacked_model_preds = np.vstack((stacked_model_preds, model_preds))\n",
    "        true_labels = np.vstack((true_labels, true))\n",
    "        \n",
    "test_preds = np.argmax(stacked_model_preds, axis=1)\n",
    "true_classes = true_labels.flatten().astype('int64')\n",
    "accuracy = np.sum(test_preds == true_classes) / len(test_preds) * 100\n",
    "\n",
    "del model\n",
    "\n",
    "print(\"Accuracy on Test set: {:.2f}%\".format(accuracy))\n",
    "print(\"\\nPredictions:\\n\")\n",
    "print(test_preds)\n",
    "print(\"\\nSince the model is not trained, the predictions are random and effectively meaningless. Highlights the importance of fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: roberta-base\n",
      "\n",
      "Learning Rate = 5.5e-06 Batch Size = 16\n",
      "\n",
      "Number of batches in train set: 175\n",
      "Number of batches in validation set: 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "175it [01:44,  1.68it/s]\n",
      "175it [01:46,  1.64it/s]\n",
      "175it [01:47,  1.62it/s]\n",
      "175it [01:49,  1.60it/s]\n",
      "175it [01:49,  1.60it/s]\n",
      "175it [01:50,  1.59it/s]\n",
      "175it [01:50,  1.59it/s]\n",
      "175it [01:50,  1.59it/s]\n",
      "175it [01:50,  1.58it/s]\n",
      "175it [01:50,  1.58it/s]\n",
      "175it [01:50,  1.58it/s]\n",
      "175it [01:50,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference time: 0.0091 seconds\n",
      "\n",
      "Early stopping triggered at epoch 12!\n"
     ]
    }
   ],
   "source": [
    "# Set seed (function called down under training)\n",
    "def seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_val)\n",
    "    else:\n",
    "        torch.manual_seed(seed_val)\n",
    "\n",
    "# Choose a model\n",
    "#model_name ='bert-base-cased'\n",
    "#model_name ='bert-large-cased'\n",
    "model_name ='roberta-base'\n",
    "#model_name ='roberta-large'\n",
    "#model_name ='xlm-roberta-base'\n",
    "#model_name = 'xlm-roberta-large'\n",
    "#model_name = 'symanto/xlm-roberta-base-snli-mnli-anli-xnli'\n",
    "#model_name = 'joeddav/xlm-roberta-large-xnli'\n",
    "\n",
    "# Initialize tokenizer\n",
    "if model_name == 'joeddav/xlm-roberta-large-xnli':\n",
    "    tokenizer = AutoTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli', use_fast=False)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Function for encoding input data\n",
    "def encode_sets(data, tokenizer):\n",
    "    kwargs = { 'truncation': True,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'padding': 'max_length',\n",
    "    'return_attention_mask': True, \n",
    "    'return_token_type_ids': True     \n",
    "    }\n",
    "    datalist = list(zip(data['premise'], data['hypothesis']))\n",
    "    tokenized = tokenizer.batch_encode_plus(datalist,**kwargs)\n",
    "    input_ids = torch.LongTensor(tokenized.input_ids)\n",
    "    attention_masks = torch.LongTensor(tokenized.attention_mask)\n",
    "    token_type_ids = torch.LongTensor(tokenized.token_type_ids)\n",
    "    return input_ids, attention_masks, token_type_ids\n",
    "\n",
    "# Get number of labels in dataset\n",
    "num_classes = dataset['label'].nunique()\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    label_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    label_names = [\n",
    "        \"antonymity\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonymity\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "# Create class weights for weighted loss\n",
    "class_counts = []\n",
    "\n",
    "for name in label_names:\n",
    "    class_counts.append(val['label_string'].value_counts()[name])\n",
    "\n",
    "total_counts = sum(class_counts)\n",
    "weights = [total_counts / class_count for class_count in class_counts]\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Set cross-entropy loss as loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "epochs_list = []\n",
    "\n",
    "# Set arrays for loss and prediction results\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "test_loss = []\n",
    "\n",
    "val_preds = []\n",
    "true_classes = []\n",
    "accuracies = []\n",
    "\n",
    "test_preds = []\n",
    "test_true_classes = []\n",
    "test_accuracies = []\n",
    "\n",
    "learning_rate = []\n",
    "batch_size = []\n",
    "\n",
    "# Number of searches\n",
    "n_searches = 1\n",
    "\n",
    "#BATCH_SIZE = [16]\n",
    "#L_RATE = [1e-6, 2e-6, 3e-6, 4e-6, 5e-6, 6e-6, 7e-6, 8e-6, 9e-6]\n",
    "\n",
    "for search in range(n_searches):\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    seed(1024)\n",
    "\n",
    "    train_loss_search = []\n",
    "    val_loss_search = []\n",
    "    test_loss_search = []\n",
    "\n",
    "    val_preds_search = []\n",
    "    true_classes_search = []\n",
    "    accuracies_search = []\n",
    "\n",
    "    test_preds_search = []\n",
    "    test_true_classes_search = []\n",
    "    test_accuracies_search = []\n",
    "\n",
    "    # Hyperparameter settings\n",
    "    #BATCH_SIZE = random.choice([8, 16, 32, 64])\n",
    "    BATCH_SIZE = 16\n",
    "    #L_RATE = 10 ** random.uniform(-5, -6)\n",
    "    L_RATE = 5.5e-06\n",
    "    MAX_LENGTH = 256\n",
    "    NUM_EPOCHS = 60\n",
    "\n",
    "    # Encode train set\n",
    "    input_ids, attention_masks, token_type_ids = encode_sets(train, tokenizer)\n",
    "    labels = torch.Tensor(train['label']).reshape(-1, 1)\n",
    "    train_tensor = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "    train_dataloader = DataLoader(train_tensor, sampler=RandomSampler(train_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Encode validation set\n",
    "    input_ids, attention_masks, token_type_ids = encode_sets(val,tokenizer)\n",
    "    labels = torch.Tensor(val['label']).reshape(-1, 1)\n",
    "    val_tensor = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "    val_dataloader = DataLoader(val_tensor, sampler=SequentialSampler(val_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "    print(f\"\\nModel: {model_name}\\n\")\n",
    "    print(f\"Learning Rate = {L_RATE} Batch Size = {BATCH_SIZE}\\n\")\n",
    "    print(f\"Number of batches in train set: {len(train_dataloader)}\")\n",
    "    print(f\"Number of batches in validation set: {len(val_dataloader)}\\n\")\n",
    "\n",
    "    # Early stopping settings\n",
    "    patience = 5\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    #stop_epochs = 0\n",
    "\n",
    "    # Initialize model\n",
    "    if model_name == \"bert-base-cased\" or model_name == 'bert-large-cased':\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes, output_hidden_states=False, output_attentions=False, problem_type=\"multi_label_classification\")\n",
    "    elif model_name == \"roberta-base\" or model_name == 'roberta-large':\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "    elif model_name == \"xlm-roberta-base\" or model_name == 'xlm-roberta-large' or ((model_name == 'symanto/xlm-roberta-base-snli-mnli-anli-xnli' or model_name == 'joeddav/xlm-roberta-large-xnli') and num_classes == 3):\n",
    "        model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "    elif (model_name == 'symanto/xlm-roberta-base-snli-mnli-anli-xnli' or model_name == 'joeddav/xlm-roberta-large-xnli') and num_classes == 11:\n",
    "        config = XLMRobertaConfig.from_pretrained(model_name, num_labels=num_classes)\n",
    "        model = XLMRobertaForSequenceClassification(config)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Load optimizer\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                lr = L_RATE, \n",
    "                eps = 1e-8\n",
    "                )\n",
    "    \n",
    "    \"\"\" # Create scheduler\n",
    "    total_steps = len(train_dataloader) * NUM_EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps = 0.1 * len(train_dataloader),\n",
    "                                                num_training_steps = total_steps)\"\"\"\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        # Training:\n",
    "\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        total_train_loss=0\n",
    "            \n",
    "        for i,batch in tqdm(enumerate(train_dataloader)):\n",
    "            # clear previous gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids, attention_masks, token_type_ids = (batch[0].to(device), \n",
    "                                                          batch[1].to(device),\n",
    "                                                          batch[2].to(device))\n",
    "            \n",
    "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "\n",
    "            #loss=outputs[0]\n",
    "            labels = torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device)\n",
    "            #labels = torch.argmax(batch[3], dim=1).to(device)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Backpropagation:\n",
    "            \n",
    "            # calculate gradients\n",
    "            loss.backward()\n",
    "            # clip the norm of the gradients to 1.0. (prevents gradient explosion)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # parameter update\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Scheduler update\n",
    "            #scheduler.step()\n",
    "\n",
    "        # Validation:\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        #torch.set_grad_enabled(False)\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for j, batch in enumerate(val_dataloader):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                input_ids, attention_masks, token_type_ids = (batch[0].to(device), \n",
    "                                                              batch[1].to(device),\n",
    "                                                              batch[2].to(device))\n",
    "                \n",
    "                start_time = time.time()\n",
    "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "                end_time = time.time()\n",
    "\n",
    "            #loss = outputs[0]\n",
    "            logits = outputs.logits\n",
    "            labels = torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device)\n",
    "            #labels = torch.argmax(batch[3], dim=1).to(device)\n",
    "            loss = criterion(logits, labels)\n",
    "            true = batch[3]\n",
    "\n",
    "            model_preds = logits.detach().cpu().numpy()\n",
    "            total_val_loss += loss.item()\n",
    "            #stop_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "            if j == 0:  # first batch\n",
    "                stacked_model_preds = model_preds\n",
    "                true_labels = true\n",
    "            else:\n",
    "                stacked_model_preds = np.vstack((stacked_model_preds, model_preds))\n",
    "                true_labels = np.vstack((true_labels, true))\n",
    "    \n",
    "        # Train loss results per epoch\n",
    "        #print(f'Training loss on epoch {epoch + 1}: {total_train_loss}')\n",
    "        train_loss_search.append(total_train_loss)\n",
    "\n",
    "        # Validation loss results per epoch\n",
    "        #print(f'\\n------------------------------------\\nValidation loss on epoch {epoch + 1}: {total_val_loss}\\n------------------------------------\\n')\n",
    "\n",
    "        val_preds_search.append(np.argmax(stacked_model_preds, axis=1))\n",
    "        true_classes_search.append(true_labels.flatten().astype('int64'))\n",
    "        accuracies_search.append(np.sum(val_preds_search[epoch] == true_classes_search[epoch])/len(val_preds_search[epoch]))\n",
    "        val_loss_search.append(total_val_loss)\n",
    "        \"\"\"\n",
    "        # Testing:\n",
    "\n",
    "        # Encode test set\n",
    "        input_ids, attention_masks, token_type_ids = encode_sets(test,tokenizer)\n",
    "        labels = torch.Tensor(test['label']).reshape(-1, 1)\n",
    "        test_tensor = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "        test_dataloader = DataLoader(test_tensor, sampler=SequentialSampler(test_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_test_loss = 0\n",
    "\n",
    "        for j, batch in enumerate(test_dataloader):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                input_ids, attention_masks, token_type_ids = (batch[0].to(device), \n",
    "                                                                batch[1].to(device),\n",
    "                                                                batch[2].to(device))\n",
    "                \n",
    "                start_time = time.time()\n",
    "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "                end_time = time.time()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            # Predicted labels\n",
    "            labels = torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device)\n",
    "            loss = criterion(logits, labels)\n",
    "            # True labels\n",
    "            test_true = batch[3]\n",
    "\n",
    "            model_preds = logits.detach().cpu().numpy()\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            if j == 0:  # first batch\n",
    "                stacked_test_preds = model_preds\n",
    "                true_labels = test_true\n",
    "            else:\n",
    "                stacked_test_preds = np.vstack((stacked_test_preds, model_preds))\n",
    "                true_labels = np.vstack((true_labels, test_true))\n",
    "\n",
    "        test_preds_search.append(np.argmax(stacked_test_preds, axis=1))\n",
    "        test_true_classes_search.append(true_labels.flatten().astype('int64'))\n",
    "        test_accuracies_search.append(np.sum(test_preds_search[epoch] == test_true_classes_search[epoch])/len(test_preds_search[epoch]))\n",
    "        test_loss_search.append(total_test_loss)\n",
    "\"\"\"\n",
    "        # Early stopping\n",
    "        if total_val_loss < best_loss:\n",
    "            best_loss = total_val_loss\n",
    "            patience_counter = 0\n",
    "            # Saving model\n",
    "            #torch.save(model, f\"roberta-base.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter == patience:\n",
    "            # Calculate inference time\n",
    "            inference_time = end_time - start_time\n",
    "            print(f\"\\nInference time: {inference_time:.4f} seconds\\n\")\n",
    "\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "    learning_rate.append(L_RATE)\n",
    "    batch_size.append(BATCH_SIZE)\n",
    "    epochs_list.append(epoch)\n",
    "    train_loss.append(train_loss_search)\n",
    "    val_loss.append(val_loss_search)\n",
    "    test_loss.append(test_loss_search)\n",
    "\n",
    "    val_preds.append(val_preds_search)\n",
    "    true_classes.append(true_classes_search)\n",
    "    accuracies.append(accuracies_search)\n",
    "    \"\"\"\n",
    "    test_preds.append(test_preds_search)\n",
    "    test_true_classes.append(test_true_classes_search)\n",
    "    test_accuracies.append(test_accuracies_search)\n",
    "    \"\"\"\n",
    "    # Clear memory\n",
    "    del model\n",
    "    del optimizer\n",
    "    #del scheduler\n",
    "    del train_tensor\n",
    "    del val_tensor\n",
    "    del train_dataloader\n",
    "    del val_dataloader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loss graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for e in range(0, len(epochs_list)):\n",
    "\n",
    "    epochs = list(range(1, epochs_list[e]+2))\n",
    "\n",
    "    # Normalize losses for visualization\n",
    "    max_train_loss = max(train_loss[e])\n",
    "    training_loss_normalized = [loss / max_train_loss for loss in train_loss[e]]\n",
    "    max_val_loss = max(val_loss[e])\n",
    "    val_loss_normalized = [loss / max_val_loss for loss in val_loss[e]]\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(epochs, training_loss_normalized, label='train loss', color='blue')\n",
    "    plt.plot(epochs, val_loss_normalized, label='val loss', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xticks(np.linspace(0, 25, 6, endpoint=True))\n",
    "    plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    # Format x-axis tick labels to display as integers\n",
    "    from matplotlib.ticker import FormatStrFormatter\n",
    "    plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "    plt.ylabel('Normalized Loss')\n",
    "    #plt.ylim(0.9, 1.0)\n",
    "    #plt.title(f'Search: {e+1} LR: {learning_rate[e]} BS: {batch_size[e]}')\n",
    "    plt.title(model_name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results of epoch with smallest test loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search 1 | Total epochs: 12 | Best epoch: 7 with loss: 174.68600142002106 and accuracy of 82.89999999999999% | LR: 5.5e-06 | BS: 16\n",
      "Highest accuracy: 82.89999999999999% at epoch 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Smallest test loss\n",
    "\n",
    "index_list = []\n",
    "\n",
    "for v in range(len(val_loss)):\n",
    "    lowest = min(val_loss[v])\n",
    "    index_lowest = val_loss[v].index(lowest)\n",
    "    acc = accuracies[v][index_lowest]\n",
    "    index_list.append(index_lowest)\n",
    "    acc_max = accuracies[v].index(max(accuracies[v]))\n",
    "    print(f\"Search {v+1} | Total epochs: {epochs_list[v]+1} | Best epoch: {index_lowest + 1} with loss: {lowest} and accuracy of {round(acc,4)*100}% | LR: {learning_rate[v]} | BS: {batch_size[v]}\")\n",
    "    print(f\"Highest accuracy: {round(max(accuracies[v]),4)*100}% at epoch {acc_max + 1}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Classification reports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification reports\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    target_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    target_names = [\n",
    "        \"antonymity\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonymity\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "classification_reports = []\n",
    "\n",
    "filename= \"cr_reports.txt\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    for e in range(len(epochs_list)):\n",
    "        for epoch in range(epochs_list[e]):\n",
    "            cr = classification_report(true_classes[e][epoch], \n",
    "                                val_preds[e][epoch],\n",
    "                                target_names=target_names, \n",
    "                                digits=4)\n",
    "            classification_reports.append(cr)\n",
    "\n",
    "            file.write(f\"Report of Search {e+1} for Epoch {epoch + 1}:\\n\\n{cr}\\n\")\n",
    "            file.write(\"-----End of Report-----\\n\\n\")\n",
    "        file.write(\"-----End of Search-----\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification report of epoch with lowest loss\n",
    "\n",
    "filename= \"cr_reports.txt\"\n",
    "\n",
    "with open(filename, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "searches = content.split(\"-----End of Search-----\\n\\n\")\n",
    "\n",
    "searches = [search for search in searches if search]\n",
    "\n",
    "reports = []\n",
    "\n",
    "for search in searches:\n",
    "    report = search.split(\"-----End of Report-----\\n\\n\")\n",
    "    reports.append(report)\n",
    "\n",
    "reports = [report for report in reports if report]\n",
    "\n",
    "i = 0\n",
    "step = 0\n",
    "\n",
    "for report in reports:\n",
    "    if step != 0:\n",
    "        i += 1\n",
    "    for r in report:\n",
    "        if report.index(r) == index_list[i]:\n",
    "            print(r)\n",
    "            step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    target_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    target_names = [\n",
    "        \"antonymity\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonymity\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "cm = confusion_matrix(true_classes[0][index_lowest], val_preds[0][index_lowest])\n",
    "\n",
    "# Plotting the confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(f'Confusion Matrix Val')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test_accuracies[0][12]\n",
    "print(f\"accuracy of {round(acc,4)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smallest test loss\n",
    "\n",
    "index_list = []\n",
    "\n",
    "for v in range(len(val_loss)):\n",
    "    #lowest = min(val_loss[v])\n",
    "    #index_lowest = test_loss[v][13]\n",
    "    acc = test_accuracies[v][12]\n",
    "    print(f\"accuracy of {round(acc,4)*100}%\")\n",
    "    #index_list.append(index_lowest)\n",
    "    #acc_max = test_accuracies[v].index(max(test_accuracies[v]))\n",
    "    #print(f\"Search {v+1} | Total epochs: {epochs_list[v]+1} | Best epoch: {index_lowest + 1} with loss: {lowest} and accuracy of {round(acc,4)*100}% | LR: {learning_rate[v]} | BS: {batch_size[v]}\")\n",
    "    #print(f\"Highest accuracy: {round(max(test_accuracies[v]),4)*100}% at epoch {acc_max + 1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification reports\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    target_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    target_names = [\n",
    "        \"antonymity\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonymity\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "classification_reports = []\n",
    "\n",
    "filename= \"test_reports.txt\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    for e in range(len(epochs_list)):\n",
    "        for epoch in range(epochs_list[e]):\n",
    "            cr = classification_report(test_true_classes[e][epoch], \n",
    "                                test_preds[e][epoch],\n",
    "                                target_names=target_names, \n",
    "                                digits=4)\n",
    "            classification_reports.append(cr)\n",
    "\n",
    "            file.write(f\"Report of Search {e+1} for Epoch {epoch + 1}:\\n\\n{cr}\\n\")\n",
    "            file.write(\"-----End of Report-----\\n\\n\")\n",
    "        file.write(\"-----End of Search-----\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    target_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    target_names = [\n",
    "        \"antonymity\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonymity\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "cm = confusion_matrix(test_true_classes[0][12], test_preds[0][12])\n",
    "\n",
    "# Plotting the confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(f'Confusion Matrix Test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Appendix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Test contradiction pre-trained models directly without fine-tuning on NLI dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli', use_fast=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained('symanto/xlm-roberta-base-snli-mnli-anli-xnli')\n",
    "\n",
    "#model_name = 'joeddav/xlm-roberta-large-xnli'\n",
    "model_name = 'symanto/xlm-roberta-base-snli-mnli-anli-xnli'\n",
    "\n",
    "# Function for encoding input data\n",
    "def encode_sets(data, tokenizer):\n",
    "    kwargs = { 'truncation': True,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'padding': 'max_length',\n",
    "    'return_attention_mask': True, \n",
    "    'return_token_type_ids': True     \n",
    "    }\n",
    "    datalist = list(zip(data['premise'], data['hypothesis']))\n",
    "    tokenized = tokenizer.batch_encode_plus(datalist,**kwargs)\n",
    "    input_ids = torch.LongTensor(tokenized.input_ids)\n",
    "    attention_masks = torch.LongTensor(tokenized.attention_mask)\n",
    "    token_type_ids = torch.LongTensor(tokenized.token_type_ids)\n",
    "    return input_ids, attention_masks, token_type_ids\n",
    "\n",
    "# Get number of labels in dataset\n",
    "num_classes = dataset['label'].nunique()\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    label_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    label_names = [\n",
    "        \"antonymity\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonymity\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "    \n",
    "# Set cross-entropy loss as loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Testing:\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# Encode test set\n",
    "input_ids, attention_masks, token_type_ids = encode_sets(test,tokenizer)\n",
    "labels = torch.Tensor(test['label']).reshape(-1, 1)\n",
    "test_tensor = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "test_dataloader = DataLoader(test_tensor, sampler=SequentialSampler(test_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "\n",
    "for j, batch in enumerate(test_dataloader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_ids, attention_masks, token_type_ids = (batch[0].to(device), \n",
    "                                                        batch[1].to(device),\n",
    "                                                        batch[2].to(device))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "        end_time = time.time()\n",
    "\n",
    "    logits = outputs.logits\n",
    "    # Predicted labels\n",
    "    labels = torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device)\n",
    "    loss = criterion(logits, labels)\n",
    "    # True labels\n",
    "    test_true = batch[3]\n",
    "\n",
    "    test_preds = logits.detach().cpu().numpy()\n",
    "    test_loss += loss.item()\n",
    "\n",
    "    if j == 0:  # first batch\n",
    "        stacked_test_preds = test_preds\n",
    "        true_labels = test_true\n",
    "    else:\n",
    "        stacked_test_preds = np.vstack((stacked_test_preds, test_preds))\n",
    "        true_labels = np.vstack((true_labels, test_true))\n",
    "\n",
    "test_preds = np.argmax(stacked_test_preds, axis=1)\n",
    "test_true_classes = true_labels.flatten().astype('int64')\n",
    "test_accuracy = np.sum(test_preds == test_true_classes)/len(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = np.sum(test_preds == test_true_classes)/len(test_preds)\n",
    "print(f\"Accuracy on test set: {round(test_accuracy,3)*100}% with loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Unfreezing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_layers(model, last_n_layers):\n",
    "    # Reverse the model layers and unfreeze the last n layers\n",
    "    layers_to_unfreeze = list(model.roberta.encoder.layer.children())[-last_n_layers:]\n",
    "    for layer in layers_to_unfreeze:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "test_preds = []\n",
    "true_classes = []\n",
    "accuracies = []\n",
    "\n",
    "phases = [2, 4, 6, 8, 10, 12]\n",
    "for index, phase in enumerate(phases):\n",
    "    unfreeze_layers(model, phase)\n",
    "    print(f\"Training with last {phase} layers unfrozen...\")\n",
    "\n",
    "    train_loss_phase = []\n",
    "    test_loss_phase = []\n",
    "\n",
    "    test_preds_phase = []\n",
    "    true_classes_phase = []\n",
    "    accuracies_phase = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        total_train_loss=0\n",
    "            \n",
    "        for i,batch in tqdm(enumerate(train_dataloader)):\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids, attention_masks, token_type_ids, labels=(batch[0].to(device), \n",
    "                                                                batch[1].to(device),\n",
    "                                                                batch[2].to(device),  \n",
    "                                                                torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device))\n",
    "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks, labels=labels)\n",
    "            loss=outputs[0]\n",
    "            #if i%10==0:\n",
    "                #print(f'loss of batch {i}: {loss}')\n",
    "            total_train_loss+=loss.item()\n",
    "\n",
    "            # Backpropagation:\n",
    "            # calculate gradients\n",
    "            loss.backward()\n",
    "            # clip the norm of the gradients to 1.0. (prevents gradient explosion)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # parameter update\n",
    "            optimizer.step()\n",
    "\n",
    "            # Scheduler update\n",
    "            #scheduler.step()\n",
    "        \n",
    "        print(f'Training loss on epoch {epoch + 1}: {total_train_loss}')\n",
    "        train_loss_phase.append(total_train_loss)\n",
    "\n",
    "        # Testing\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        torch.set_grad_enabled(False)\n",
    "        total_test_loss = 0\n",
    "\n",
    "        for j, batch in enumerate(val_dataloader):\n",
    "            input_ids, attention_masks, token_type_ids, labels=(batch[0].to(device), \n",
    "                                                                batch[1].to(device),\n",
    "                                                                batch[2].to(device), \n",
    "                                                                torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device))\n",
    "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks, labels = labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            logits = outputs.logits\n",
    "            true = batch[3]\n",
    "\n",
    "            val_preds = logits.detach().cpu().numpy()\n",
    "            total_test_loss += loss.item()\n",
    "            if j == 0:  # first batch\n",
    "                stacked_val_preds = val_preds\n",
    "                true_labels = true\n",
    "\n",
    "            else:\n",
    "                stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n",
    "                true_labels = np.vstack((true_labels, true))\n",
    "                #stacked_val_preds.extend(val_preds)\n",
    "            #print(len(stacked_val_preds))\n",
    "        \n",
    "        print(f'\\n------------------------------------\\nTest loss on epoch {epoch + 1}: {total_test_loss}\\n------------------------------------\\n')\n",
    "\n",
    "        # Prediction results per epoch\n",
    "\n",
    "        test_preds_phase.append(np.argmax(stacked_val_preds, axis=1))\n",
    "        true_classes_phase.append(true_labels.flatten().astype('int64'))\n",
    "        accuracies_phase.append(np.sum(test_preds_phase[epoch] == true_classes_phase[epoch])/len(test_preds_phase[epoch]))\n",
    "\n",
    "        # Test loss per epoch\n",
    "\n",
    "        test_loss_phase.append(total_test_loss)\n",
    "    \n",
    "    train_loss.append(train_loss_phase)\n",
    "    test_loss.append(test_loss_phase)\n",
    "\n",
    "    test_preds.append(test_preds_phase)\n",
    "    true_classes.append(true_classes_phase)\n",
    "    accuracies.append(accuracies_phase)\n",
    "\n",
    "    NUM_EPOCHS = NUM_EPOCHS + 5\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\n",
    "    \"antonymity\",\n",
    "    \"entailment\",\n",
    "    \"factive_antonymity\",\n",
    "    \"factive_embedding_verb\",\n",
    "    \"lexical\",\n",
    "    \"negation\",\n",
    "    \"neutral\",\n",
    "    \"numeric\",\n",
    "    \"structure\",\n",
    "    \"temporal\",\n",
    "    \"worldknowledge\"\n",
    "]   \n",
    "\n",
    "classification_reports = []\n",
    "\n",
    "filename= \"test.txt\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    for epoch in range(stop_epochs-1):\n",
    "        cr = classification_report(true_classes[epoch], \n",
    "                            val_preds[epoch],\n",
    "                            target_names=target_names, \n",
    "                            digits=4)\n",
    "        classification_reports.append(cr)\n",
    "\n",
    "        file.write(f\"Report for Epoch {epoch + 1}:\\n{cr}\\n\")\n",
    "        file.write(\"-----End of Report-----\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= \"test.txt\"\n",
    "\n",
    "with open(filename, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Split the content back into individual reports using the delimiter\n",
    "# This creates a list where each element is a report\n",
    "reports = content.split(\"-----End of Report-----\\n\\n\")\n",
    "\n",
    "# Remove any empty strings in case they exist\n",
    "reports = [report for report in reports if report]\n",
    "\n",
    "# Iterate over the reports and print them\n",
    "for report in reports[index_lowest:index_lowest+1]:\n",
    "    print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
