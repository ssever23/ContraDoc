{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "# huggingface libraries \n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, XLMRobertaConfig\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import TFAutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset, TensorDataset\n",
    "\n",
    "# additional libraries\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "\n",
    "#viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the logging level to error to suppress warnings (and info messages)\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface authetication for model download\n",
    "\n",
    "#from huggingface_hub import login\n",
    "#login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Initiate cuda**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: cuda\n",
      "There are 1 GPU(s) available.\n",
      "GPU is: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device type:', device)\n",
    "print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "print('GPU is:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Load NLI dataset into dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/home/ssever/contradiction-detection/data/csv_files/nli_data_set.csv')\n",
    "\n",
    "#dataset = dataset[dataset.label_string != 'worldknowledge']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transform dataset to 3 labels (Use only for 3-label classification!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_map = {\n",
    "    \"antonymity\": \"contradiction\",\n",
    "    \"factive_antonymity\": \"contradiction\",\n",
    "    \"factive_embedding_verb\": \"contradiction\",\n",
    "    \"lexical\": \"contradiction\",\n",
    "    \"negation\": \"contradiction\",\n",
    "    \"numeric\": \"contradiction\",\n",
    "    \"structure\": \"contradiction\",\n",
    "    \"temporal\": \"contradiction\",\n",
    "    \"worldknowledge\": \"contradiction\"\n",
    "}\n",
    "\n",
    "# Replace all labels except 'neutral' and 'entailment' with 'contradiction'\n",
    "dataset['label_string'] = dataset['label_string'].replace(replacement_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify unique words and sort them to maintain consistency\n",
    "unique_words = sorted(dataset['label_string'].unique())\n",
    "\n",
    "# Create a mapping from words to digits\n",
    "word_to_digit = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "\n",
    "# Apply the mapping to the 'label' column\n",
    "dataset['label'] = dataset['label_string'].map(word_to_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create holdout test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: (3097, 5) \n",
      "\n",
      "Test size: (345, 5)\n"
     ]
    }
   ],
   "source": [
    "data, test = train_test_split(dataset, stratify=dataset.label.values, \n",
    "                                                  random_state=42, # note that holdout test set distribution is permanently set\n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# check the number of rows and columns after split\n",
    "print(\"Data size: {} \\n\".format(data.shape))\n",
    "print(\"Test size: {}\".format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create train and validation data sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (2787, 5) \n",
      "\n",
      "Validation size: (310, 5)\n"
     ]
    }
   ],
   "source": [
    "train, val = train_test_split(data, stratify=data.label.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# check the number of rows and columns after split\n",
    "print(\"Train size: {} \\n\".format(train.shape))\n",
    "print(\"Validation size: {}\".format(val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Check number of classes again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for number of unique labels\n",
    "dataset['label_string'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of classes: 11\n",
      "\n",
      "Model: roberta-base\n",
      "\n",
      "\n",
      "Learning Rate = 5.5e-06 Batch Size = 16\n",
      "\n",
      "Number of batches in train set: 175\n",
      "Number of batches in validation set: 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "175it [01:35,  1.83it/s]\n",
      "175it [01:39,  1.76it/s]\n",
      "175it [01:42,  1.70it/s]\n",
      "175it [01:42,  1.70it/s]\n",
      "175it [01:42,  1.70it/s]\n",
      "175it [01:45,  1.67it/s]\n",
      "175it [01:44,  1.68it/s]\n",
      "175it [01:44,  1.68it/s]\n",
      "175it [01:44,  1.68it/s]\n",
      "175it [01:44,  1.68it/s]\n",
      "175it [01:44,  1.68it/s]\n",
      "175it [01:43,  1.68it/s]\n",
      "175it [01:43,  1.68it/s]\n",
      "175it [01:46,  1.64it/s]\n",
      "175it [01:47,  1.63it/s]\n",
      "175it [01:45,  1.66it/s]\n",
      "175it [01:45,  1.66it/s]\n",
      "175it [01:45,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference time: 0.0075 seconds\n",
      "\n",
      "Early stopping triggered at epoch 18!\n",
      "\n",
      "Total time for training: 32.17 minutes\n"
     ]
    }
   ],
   "source": [
    "# Set seed (function called down under training)\n",
    "def seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_val)\n",
    "    else:\n",
    "        torch.manual_seed(seed_val)\n",
    "\n",
    "# Choose a model\n",
    "#model_name ='bert-base-cased'\n",
    "#model_name ='bert-large-cased'\n",
    "model_name ='roberta-base'\n",
    "#model_name ='roberta-large'\n",
    "#model_name ='xlm-roberta-base'\n",
    "#model_name = 'xlm-roberta-large'\n",
    "#model_name = 'symanto/xlm-roberta-base-snli-mnli-anli-xnli'\n",
    "#model_name = 'joeddav/xlm-roberta-large-xnli'\n",
    "\n",
    "# Initialize tokenizer\n",
    "if model_name == 'joeddav/xlm-roberta-large-xnli':\n",
    "    tokenizer = AutoTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli', use_fast=False)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Function for encoding input data\n",
    "def encode_sets(data, tokenizer):\n",
    "    kwargs = { 'truncation': True,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'padding': 'max_length',\n",
    "    'return_attention_mask': True, \n",
    "    'return_token_type_ids': True     \n",
    "    }\n",
    "    datalist = list(zip(data['premise'], data['hypothesis']))\n",
    "    tokenized = tokenizer.batch_encode_plus(datalist,**kwargs)\n",
    "    input_ids = torch.LongTensor(tokenized.input_ids)\n",
    "    attention_masks = torch.LongTensor(tokenized.attention_mask)\n",
    "    token_type_ids = torch.LongTensor(tokenized.token_type_ids)\n",
    "    return input_ids, attention_masks, token_type_ids\n",
    "\n",
    "# Get number of labels in dataset\n",
    "num_classes = dataset['label'].nunique()\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    label_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    label_names = [\n",
    "        \"antonymity\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonymity\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "# Create class weights for weighted loss\n",
    "class_counts = []\n",
    "\n",
    "for name in label_names:\n",
    "    class_counts.append(val['label_string'].value_counts()[name])\n",
    "\n",
    "total_counts = sum(class_counts)\n",
    "weights = [total_counts / class_count for class_count in class_counts]\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Set cross-entropy loss as loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "epochs_list = []\n",
    "\n",
    "# Set arrays for loss and prediction results\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "test_loss = []\n",
    "\n",
    "val_preds = []\n",
    "true_classes = []\n",
    "accuracies = []\n",
    "\n",
    "test_preds = []\n",
    "test_true_classes = []\n",
    "test_accuracies = []\n",
    "\n",
    "learning_rate = []\n",
    "batch_size = []\n",
    "\n",
    "# Number of searches\n",
    "n_searches = 1\n",
    "\n",
    "#BATCH_SIZE = [16]\n",
    "#L_RATE = [1e-6, 2e-6, 3e-6, 4e-6, 5e-6, 6e-6, 7e-6, 8e-6, 9e-6]\n",
    "\n",
    "for search in range(n_searches):\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    seed(1024)\n",
    "\n",
    "    train_loss_search = []\n",
    "    val_loss_search = []\n",
    "    test_loss_search = []\n",
    "\n",
    "    val_preds_search = []\n",
    "    true_classes_search = []\n",
    "    accuracies_search = []\n",
    "\n",
    "    test_preds_search = []\n",
    "    test_true_classes_search = []\n",
    "    test_accuracies_search = []\n",
    "\n",
    "    # Hyperparameter settings\n",
    "    #BATCH_SIZE = random.choice([8, 16, 32, 64])\n",
    "    BATCH_SIZE = 16\n",
    "    #L_RATE = 10 ** random.uniform(-5, -6)\n",
    "    L_RATE = 5.5e-06\n",
    "    #L_RATE = 1e-05\n",
    "    MAX_LENGTH = 256\n",
    "    NUM_EPOCHS = 80\n",
    "\n",
    "    # Encode train set\n",
    "    input_ids, attention_masks, token_type_ids = encode_sets(train, tokenizer)\n",
    "    labels = torch.Tensor(train['label']).reshape(-1, 1)\n",
    "    train_tensor = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "    train_dataloader = DataLoader(train_tensor, sampler=RandomSampler(train_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Encode validation set\n",
    "    input_ids, attention_masks, token_type_ids = encode_sets(val,tokenizer)\n",
    "    labels = torch.Tensor(val['label']).reshape(-1, 1)\n",
    "    val_tensor = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "    val_dataloader = DataLoader(val_tensor, sampler=SequentialSampler(val_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Early stopping settings\n",
    "    patience = 10\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    #stop_epochs = 0\n",
    "\n",
    "    # Initialize model\n",
    "    if model_name == \"bert-base-cased\" or model_name == 'bert-large-cased':\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes, output_hidden_states=False, output_attentions=False, problem_type=\"multi_label_classification\")\n",
    "    elif model_name == \"roberta-base\" or model_name == 'roberta-large':\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "    elif model_name == \"xlm-roberta-base\" or model_name == 'xlm-roberta-large' or ((model_name == 'symanto/xlm-roberta-base-snli-mnli-anli-xnli' or model_name == 'joeddav/xlm-roberta-large-xnli') and num_classes == 3):\n",
    "        model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "    elif (model_name == 'symanto/xlm-roberta-base-snli-mnli-anli-xnli' or model_name == 'joeddav/xlm-roberta-large-xnli') and num_classes == 11:\n",
    "        config = XLMRobertaConfig.from_pretrained(model_name, num_labels=num_classes)\n",
    "        model = XLMRobertaForSequenceClassification(config)\n",
    "        \n",
    "    \"\"\"\n",
    "    # Define LoRA configuration\n",
    "    r = 16\n",
    "    lora_alpha = r*2\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=r,            # Rank of the low-rank matrices\n",
    "    lora_alpha=lora_alpha,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout rate\n",
    "    target_modules=[\"query\", \"key\", \"value\"]  # Apply LoRA only to attention matrices\n",
    ")\n",
    "\n",
    "    # Apply LoRA configuration to model\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move model to device (GPU)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Print model and training details\n",
    "\n",
    "    print(f\"\\nNumber of classes: {num_classes}\")\n",
    "    print(f\"\\nModel: {model_name}\\n\")\n",
    "    #model.print_trainable_parameters()\n",
    "    print(f\"\\nLearning Rate = {L_RATE} Batch Size = {BATCH_SIZE}\\n\")\n",
    "    print(f\"Number of batches in train set: {len(train_dataloader)}\")\n",
    "    print(f\"Number of batches in validation set: {len(val_dataloader)}\\n\")\n",
    "\n",
    "    # Load optimizer\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                lr = L_RATE, \n",
    "                eps = 1e-8\n",
    "                )\n",
    "    \n",
    "    \"\"\" # Create scheduler\n",
    "    total_steps = len(train_dataloader) * NUM_EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps = 0.1 * len(train_dataloader),\n",
    "                                                num_training_steps = total_steps)\"\"\"\n",
    "\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        # Training:\n",
    "\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        total_train_loss=0\n",
    "            \n",
    "        for i,batch in tqdm(enumerate(train_dataloader)):\n",
    "            # clear previous gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids, attention_masks, token_type_ids = (batch[0].to(device), \n",
    "                                                          batch[1].to(device),\n",
    "                                                          batch[2].to(device))\n",
    "            \n",
    "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "\n",
    "            #loss=outputs[0]\n",
    "            labels = torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device)\n",
    "            #labels = torch.argmax(batch[3], dim=1).to(device)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Backpropagation:\n",
    "            \n",
    "            # calculate gradients\n",
    "            loss.backward()\n",
    "            # clip the norm of the gradients to 1.0. (prevents gradient explosion)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # parameter update\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Scheduler update\n",
    "            #scheduler.step()\n",
    "\n",
    "        # Validation:\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        #torch.set_grad_enabled(False)\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for j, batch in enumerate(val_dataloader):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                input_ids, attention_masks, token_type_ids = (batch[0].to(device), \n",
    "                                                              batch[1].to(device),\n",
    "                                                              batch[2].to(device))\n",
    "                \n",
    "                start_time = time.time()\n",
    "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "                end_time = time.time()\n",
    "\n",
    "            #loss = outputs[0]\n",
    "            logits = outputs.logits\n",
    "            labels = torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device)\n",
    "            #labels = torch.argmax(batch[3], dim=1).to(device)\n",
    "            loss = criterion(logits, labels)\n",
    "            true = batch[3]\n",
    "\n",
    "            model_preds = logits.detach().cpu().numpy()\n",
    "            total_val_loss += loss.item()\n",
    "            #stop_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "            if j == 0:  # first batch\n",
    "                stacked_model_preds = model_preds\n",
    "                true_labels = true\n",
    "            else:\n",
    "                stacked_model_preds = np.vstack((stacked_model_preds, model_preds))\n",
    "                true_labels = np.vstack((true_labels, true))\n",
    "    \n",
    "        # Train loss results per epoch\n",
    "        #print(f'Training loss on epoch {epoch + 1}: {total_train_loss}')\n",
    "        train_loss_search.append(total_train_loss)\n",
    "\n",
    "        # Validation loss results per epoch\n",
    "        #print(f'\\n------------------------------------\\nValidation loss on epoch {epoch + 1}: {total_val_loss}\\n------------------------------------\\n')\n",
    "\n",
    "        val_preds_search.append(np.argmax(stacked_model_preds, axis=1))\n",
    "        true_classes_search.append(true_labels.flatten().astype('int64'))\n",
    "        accuracies_search.append(np.sum(val_preds_search[epoch] == true_classes_search[epoch])/len(val_preds_search[epoch]))\n",
    "        val_loss_search.append(total_val_loss)\n",
    "        \"\"\"\n",
    "        # Testing:\n",
    "\n",
    "        # Encode test set\n",
    "        input_ids, attention_masks, token_type_ids = encode_sets(test,tokenizer)\n",
    "        labels = torch.Tensor(test['label']).reshape(-1, 1)\n",
    "        test_tensor = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "        test_dataloader = DataLoader(test_tensor, sampler=SequentialSampler(test_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_test_loss = 0\n",
    "\n",
    "        for j, batch in enumerate(test_dataloader):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                input_ids, attention_masks, token_type_ids = (batch[0].to(device), \n",
    "                                                                batch[1].to(device),\n",
    "                                                                batch[2].to(device))\n",
    "                \n",
    "                start_time = time.time()\n",
    "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "                end_time = time.time()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            # Predicted labels\n",
    "            labels = torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device)\n",
    "            loss = criterion(logits, labels)\n",
    "            # True labels\n",
    "            test_true = batch[3]\n",
    "\n",
    "            model_preds = logits.detach().cpu().numpy()\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            if j == 0:  # first batch\n",
    "                stacked_test_preds = model_preds\n",
    "                true_labels = test_true\n",
    "            else:\n",
    "                stacked_test_preds = np.vstack((stacked_test_preds, model_preds))\n",
    "                true_labels = np.vstack((true_labels, test_true))\n",
    "\n",
    "        test_preds_search.append(np.argmax(stacked_test_preds, axis=1))\n",
    "        test_true_classes_search.append(true_labels.flatten().astype('int64'))\n",
    "        test_accuracies_search.append(np.sum(test_preds_search[epoch] == test_true_classes_search[epoch])/len(test_preds_search[epoch]))\n",
    "        test_loss_search.append(total_test_loss)\n",
    "\"\"\"\n",
    "        # Early stopping\n",
    "        if total_val_loss < best_loss:\n",
    "            best_loss = total_val_loss\n",
    "            patience_counter = 0\n",
    "            # Saving model\n",
    "            #torch.save(model, f\"roberta-base.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter == patience:\n",
    "            # Calculate inference time\n",
    "            inference_time = end_time - start_time\n",
    "            print(f\"\\nInference time: {inference_time:.4f} seconds\\n\")\n",
    "\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
    "            break\n",
    "        \n",
    "    total_end_time = time.time()\n",
    "    print(f\"\\nTotal time for training: {(total_end_time - total_start_time)/60:.2f} minutes\")\n",
    "    \n",
    "\n",
    "    learning_rate.append(L_RATE)\n",
    "    batch_size.append(BATCH_SIZE)\n",
    "    epochs_list.append(epoch)\n",
    "    train_loss.append(train_loss_search)\n",
    "    val_loss.append(val_loss_search)\n",
    "    test_loss.append(test_loss_search)\n",
    "\n",
    "    val_preds.append(val_preds_search)\n",
    "    true_classes.append(true_classes_search)\n",
    "    accuracies.append(accuracies_search)\n",
    "    \"\"\"\n",
    "    test_preds.append(test_preds_search)\n",
    "    test_true_classes.append(test_true_classes_search)\n",
    "    test_accuracies.append(test_accuracies_search)\n",
    "    \"\"\"\n",
    "    # Clear memory\n",
    "    del model\n",
    "    del optimizer\n",
    "    #del scheduler\n",
    "    del train_tensor\n",
    "    del val_tensor\n",
    "    del train_dataloader\n",
    "    del val_dataloader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loss graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAAE8CAYAAACb7Fv6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABTJUlEQVR4nO3dd3yNd//H8dfJHiRBZCCSEFskBGmMqjbEyo0OWjRoUaM1UqraWu3vRtXq0Co1e1ftVVtjFI0tpWoLSUsSM5FBxrl+f1wcUgk5WVfG5/l4XA/nXOe6zvmcy2nfruv6Dp2iKApCCCGEyDUTrQsQQgghijsJUyGEECKPJEyFEEKIPJIwFUIIIfJIwlQIIYTIIwlTIYQQIo8kTIUQQog8kjAVQggh8kjCVAghhMgjCVMhComHhwedOnXSuow8e+GFF6hfv77WZQhRpEiYClECXb16lQkTJhAREaF1KUKUChKmQpRAV69eZeLEiRKmQhQSCVMh8iApKUnrEjJJT08nNTVV6zKEKHUkTIXIoQkTJqDT6fjrr7/o0aMH5cqVo0WLFqSnp/PZZ59RvXp1LC0t8fDw4KOPPuL+/ftZvs/27dvx9fXFysqKunXrsmbNmie2uXPnDsOHD8fNzQ1LS0u8vLz4/PPP0ev1hm0uX76MTqdj2rRpzJo1y/D53377LU2aNAGgb9++6HQ6dDodixYtAmDv3r289tprVK1aFUtLS9zc3BgxYgQpKSlGHY+jR4/SrFkzrK2t8fT0ZM6cOZleT01NZdy4cfj5+WFvb4+trS0tW7Zk165dT7zXsmXL8PPzo2zZstjZ2eHt7c2XX35p9DERQis6mYJNiJyZMGECEydOpG7dutSoUYOgoCAUReHQoUMsXryYV199ldatW3Pw4EGWLFlCly5dWLt2rWF/Dw8PLC0tiYuLY+DAgTg5ObFw4UJOnTrF1q1badOmDQDJyckEBATwzz//8M4771C1alV+//13fvzxR4YOHcqsWbMANUw9PT2pW7cu9+7dY8CAAVhaWtK1a1eWLFnCuHHjGDBgAC1btgSgWbNmVKtWjaFDh3Lx4kVatGhBhQoVOHToEIsWLaJr166sXLnymcfhhRde4Pz586Snp9OtWzdq1qzJihUr2LdvH/Pnz+ett94C4MaNGzRo0IA33niDGjVqcPfuXebPn8+lS5c4dOgQvr6+AOzYsYO2bdvy0ksv8fLLLwNw+vRpYmNjWbFihVHHRAjNKEKIHBk/frwCKG+88YZhXUREhAIo/fr1y7TtyJEjFUDZuXOnYZ27u7sCKKtXrzasi4+PV1xdXZWGDRsa1n322WeKra2tcu7cuUzv+eGHHyqmpqZKVFSUoiiKEhkZqQCKnZ2dEhcXl2nbw4cPK4CycOHCJ75HcnLyE+smT56s6HQ65cqVK888Dq1atVIAZfr06YZ19+/fV3x9fRUnJyclNTVVURRFSU9PV+7fv59p39u3byvOzs7KW2+9ZVg3bNgwxc7OTklPT8/2M3N6TITQilzmFcJIAwcONDzevHkzAKGhoZm2ef/99wHYtGlTpvWVKlWia9euhud2dnaEhIRw/PhxYmJiAFi5ciUtW7akXLly3Lhxw7AEBgaSkZHBb7/9luk9X3nlFSpWrJjj+q2trQ2Pk5KSuHHjBs2aNUNRFI4fP56j9zAzM+Odd94xPLewsOCdd94hLi6Oo0ePAmBqaoqFhQUAer2eW7dukZ6eTuPGjTl27JhhXwcHB5KSktixY0e2n2fsMRGisJlpXYAQxY2np6fh8ZUrVzAxMcHLyyvTNi4uLjg4OHDlypVM6728vNDpdJnW1axZE1Av27q4uHD+/HlOnDiRbUDGxcVlW09OREVFMW7cODZs2MDt27czvRYfHw9ASkqK4fHj3+mhSpUqYWtrm+33eO655wBYvHgx06dP58yZM6SlpWVZ8+DBg1mxYgXt27encuXKtG3blm7dutGuXTvDNsYeEyEKm4SpEEZ6/MzuoX8HZF7o9XratGnDBx98kOXrD0PrafVkJyMjgzZt2nDr1i1Gjx5N7dq1sbW15Z9//qFPnz6GxjzLly+nb9++mfZVjGxe8b///Y8+ffrQpUsXRo0ahZOTE6ampkyePJmLFy8atnNyciIiIoJt27axZcsWtmzZwsKFCwkJCWHx4sWA8cdEiMImYSpEHri7u6PX6zl//jx16tQxrI+NjeXOnTu4u7tn2v7ChQsoipIpfM+dOweoDZQAqlevTmJiIoGBgbmuK7twP3nyJOfOnWPx4sWEhIQY1v/7EmtQUNBTL7tevXqVpKSkTGen//4eq1atolq1aqxZsyZTPePHj3/i/SwsLAgODiY4OBi9Xs/gwYP5/vvvGTt2LF5eXvlyTIQoSHLPVIg86NChA8ATrUlnzJgBQMeOHTOtv3r1aqYWvgkJCSxZsgRfX1/DZdRu3boRHh7Otm3bnvi8O3fukJ6e/sy6HobcnTt3Mq03NTUFMp9lKoryRDcUV1dXAgMDMy2PS09P5/vvvzc8T01N5fvvv6dixYr4+fll+1kHDx4kPDw803vdvHkz03MTExMaNGgAYOhelB/HRIiCJGemQuSBj48PvXv3Zu7cudy5c4dWrVoZusp06dKF1q1bZ9q+Zs2avP322xw+fBhnZ2cWLFhAbGwsCxcuNGwzatQoNmzYQKdOnejTpw9+fn4kJSVx8uRJVq1axeXLl3F0dHxqXdWrV8fBwYE5c+ZQtmxZbG1t8ff3p3bt2lSvXp2RI0fyzz//YGdnx+rVq5+4d/oslSpV4vPPP+fy5cvUrFmT5cuXExERwdy5czE3NwegU6dOrFmzhq5du9KxY0ciIyOZM2cOdevWJTEx0fBe/fr149atW7z44otUqVKFK1eu8PXXX+Pr62s428+PYyJEgdKyKbEQxcnDrjHXr1/PtD4tLU2ZOHGi4unpqZibmytubm7KmDFjlHv37mXazt3dXenYsaOybds2pUGDBoqlpaVSu3ZtZeXKlU981t27d5UxY8YoXl5eioWFheLo6Kg0a9ZMmTZtmqHrycOuMV988UWW9a5fv16pW7euYmZmlqmbzF9//aUEBgYqZcqUURwdHZX+/fsrf/zxR7Zdaf6tVatWSr169ZQjR44oAQEBipWVleLu7q588803mbbT6/XKpEmTFHd3d8XS0lJp2LChsnHjRqV3796Ku7u7YbtVq1Ypbdu2VZycnBQLCwulatWqyjvvvKNcu3bN6GMihFZk0AYhhBAij+SeqRBCCJFHEqZCCCFEHkmYCiGEEHkkYSqEEELkkYSpEEIIkUcSpkIIIUQelbpBG/R6PVevXqVs2bL5Op6qEEKI4kVRFO7evUulSpUwMcnbuWWpC9OrV6/i5uamdRlCCCGKiOjoaKpUqZKn9yh1YVq2bFlAPXh2dnYaVyOEEEIrCQkJuLm5GXIhL0pdmD68tGtnZydhKoQQIl9u+UkDJCGEECKPJEyFEEKIPJIwFUIIIfJI0zD97bffCA4OplKlSuh0OtatW/fMfXbv3k2jRo2wtLTEy8uLRYsWFXidQgghxNNoGqZJSUn4+Pgwe/bsHG0fGRlJx44dad26NREREQwfPpx+/fqxbdu2Aq5UCCGEyJ6mrXnbt29P+/btc7z9nDlz8PT0ZPr06QDUqVOHffv2MXPmTIKCggqqzCek3kvDwsq80D5PCCFE0Vas7pmGh4cTGBiYaV1QUBDh4eHZ7nP//n0SEhIyLbkVsW0nf87w58jst3P9HkIIIUqeYhWmMTExODs7Z1rn7OxMQkICKSkpWe4zefJk7O3tDUteRj/SmVlS3+UQ9ezXk3D7Xq7fRwghRMlSrMI0N8aMGUN8fLxhiY6OzvV7NWgdQExCZextEghfuz0fqxRCCFGcFaswdXFxITY2NtO62NhY7OzssLa2znIfS0tLw2hHeR31SGdiwj8mrwKgv7wi1+8jhBCiZClWYRoQEEBYWFimdTt27CAgIKDQavB8oRsAzd038OcfcqlXCCGExmGamJhIREQEERERgNr1JSIigqioKEC9RBsSEmLYfuDAgVy6dIkPPviAM2fO8O2337JixQpGjBhRaDWXr/EcN5KrYGd9lwNrpUuOEEIIjcP0yJEjNGzYkIYNGwIQGhpKw4YNGTduHADXrl0zBCuAp6cnmzZtYseOHfj4+DB9+nR++OGHQu0Wg86ExHKvAWAfv4J7cnIqhBClnk5RFEXrIgpTQkIC9vb2xMfH5/r+aUbsAUzDAribUoatNnG89kbW92uFEEIUXfmRBw8Vq3umRYWpkz930qpS1jqRP3/dqnU5QgghNCZhmhs6HTp39VJvbesVXLqkcT1CCCE0JWGaS/b11Va9wY1+4ceFyRpXI4QQQksSprlVoQlJuFPGKom/D28hPV3rgoQQQmhFwjS3dDosa6iXegNrrkAmrhFCiNJLwjQPzKqpl3o7+W5kiVzqFUKIUkvCNC/KNybVwgNbq2SUfzYTE6N1QUIIIbQgYZoXOh0W1dWz01ebrGDxYo3rEUIIoQkJ07xyf3Cpt+FGflqcROkaAkMIIQRImOZduUbobaphY5lCbbtN/Pab1gUJIYQobBKmeaXTYeKhnp12e24FP/ygcT1CCCEKnYRpfqiqhmlH301s+SWRO3e0LUcIIUThkjDND+V8UcpUx9riHoF1NrJ0qdYFCSGEKEwSpvlBp0P34Oz0Nf+VcqlXCCFKGQnT/PKgVW8H382cP32XY8c0rkcIIUShkTDNLw4+ULYG1hb36NRwo5ydCiFEKSJhml90OkNDpG7+K/jpJ0iWEQaFEKJUkDDNT1UfXurdgpKWwKpVGtcjhBCiUEiY5icHb7CrhaX5fYIb/iKXeoUQopSQMM1Pj13q7f7cCvbuhbNnNa5JCCFEgZMwzW9V1TlO2/tuxc46ngULNK5HCCFEgZMwzW/29cGuNuamqfzHbwOLFkFamtZFCSGEKEgSpvntsUu9vVquJC4ONm7UuCYhhBAFSsK0IDwI05fqbsPe5o40RBJCiBJOwrQgONQD+7qYmaTyn0Yb2LoVoqO1LkoIIURBkTAtKA/OTgd1WIFeD4sWaVuOEEKIgiNhWlAetOptWnU7Dja3mT8f9HqNaxJCCFEgJEwLin1dsK+PqS6NN1qu58oVCAvTuighhBAFQcK0ID241Duk0woA5s/XshghhBAFRcK0ID241Fu3/A7K2d5i7Vq4cUPjmoQQBePeDbhzSusqhEYkTAuSfW1w8EZHOsNeWUdqKvzvf1oXJYQoEEeGwNZGcGmJ1pUIDUiYFrQHl3r7vrgSgB9+AEXRsiAhRL67shyiVoCiV7vGiVJH8zCdPXs2Hh4eWFlZ4e/vz6FDh566/axZs6hVqxbW1ta4ubkxYsQI7t27V0jV5sKDS71uFr9S2fEmp05BeLjGNQkh8k9KDBwerD6u9zGU99O2HqEJTcN0+fLlhIaGMn78eI4dO4aPjw9BQUHExcVluf3SpUv58MMPGT9+PKdPn2b+/PksX76cjz76qJArN4JdLXDwQaek89k76wCYOVPbkoQQ+URR4NAASL0F5RpC/Y+1rkhoRNMwnTFjBv3796dv377UrVuXOXPmYGNjw4Jsplr5/fffad68OT169MDDw4O2bdvyxhtvPPNsVnPu6qXeV5uqrXrXrIGLF7UsSAiRLyKXwD+/gIkFBCwGE3OtKxIa0SxMU1NTOXr0KIGBgY+KMTEhMDCQ8GyugzZr1oyjR48awvPSpUts3ryZDh06ZPs59+/fJyEhIdNS6NzUS71lk8Lo1vkGer2cnQpR7CVFw9Fh6mPvieDgrW09QlOahemNGzfIyMjA2dk503pnZ2diYmKy3KdHjx58+umntGjRAnNzc6pXr84LL7zw1Mu8kydPxt7e3rC4ubnl6/fIEbsa6iUgJYOJ/dYCsGAB3LxZ+KUIIfKBosDBfpAWDxX8oc5IrSsSGtO8AZIxdu/ezaRJk/j22285duwYa9asYdOmTXz22WfZ7jNmzBji4+MNS7RWI84/aNVby3oFDRtCSgp89502pQgh8ujCXIjZDqZWDy7vmmldkdCYZmHq6OiIqakpsbGxmdbHxsbi4uKS5T5jx47lzTffpF+/fnh7e9O1a1cmTZrE5MmT0Wcz8K2lpSV2dnaZFk08aNWri9vJxyOvA/D111CUGyILIbKQeAmOv68+9pmsNjIUpZ5mYWphYYGfnx9hjw1Yq9frCQsLIyAgIMt9kpOTMTHJXLKpqSkASlHvvFm2utpkXtHTpcFi3NwgLg5+/FHrwoQQOabo4cBbkJ4ETs9DraFaVySKCE0v84aGhjJv3jwWL17M6dOnGTRoEElJSfTt2xeAkJAQxowZY9g+ODiY7777jmXLlhEZGcmOHTsYO3YswcHBhlAt0rwGAmB64WtCR6QDMH26zCYjRLFx9muI2wNmtvDcQtAVqztlogBpeqG/e/fuXL9+nXHjxhETE4Ovry9bt241NEqKiorKdCb6ySefoNPp+OSTT/jnn3+oWLEiwcHB/Pe//9XqKxjHoyf88REkR/FOhzVMmNiNs2dh0yYIDta6OCHEUyWchT8+VB83nAZlqmlbjyhSdEqRvz6avxISErC3tyc+Pl6b+6cnxsOfn0IFf0bvPMDUqdCyJfz2W+GXIoTIIX0G7GgBNw+ASxtovQ10Oq2rEnmUn3kg1ygKW43BagfvmwcZ2Tscc3PYuxcOHtS6MCFEts5MU4PU3A7850uQiicYHaaLFy9m06ZNhucffPABDg4ONGvWjCtXruRrcSWStbN6uReoeHMGPXqoq6dP17AmIUT27vwJJ8apj/2+BFsN+qqLIs/oMJ00aRLW1tYAhIeHM3v2bKZOnYqjoyMjRozI9wJLpNoPjtPfa/jwvcsArF4Nly5pV5IQIgv6NAgPAX0qVA4Gz95aVySKKKPDNDo6Gi8vLwDWrVvHK6+8woABA5g8eTJ79+7N9wJLJAdvcAkERU9tviIoCBliUIii6NQkuH0cLMpD07lyeVdky+gwLVOmDDcfjIO3fft22rRpA4CVlRUpKSn5W11JVjtU/fPiD3z4vjpesAwxKEQRcusY/Pl/6uPGs8E668FkhIBchGmbNm3o168f/fr149y5c4ZB5k+dOoWHh0d+11dyuQaBXW1Iv0urKvPx9YXkZJgzR+vChBBk3Fcv7yrp6uhl7t21rkgUcUaH6ezZswkICOD69eusXr2aChUqAHD06FHeeOONfC+wxNKZGO6d6s59xaiR6iAOMsSgEEXAyQkQfwqsnKDxt3J5VzyT9DPVUnoKrHeD+zdJD1hJtVavEh0N8+ZBv37aliZEqXXjAOxorg4d2HItuHXRuiJRQDTtZ7p161b27dtneD579mx8fX3p0aMHt2/fzlMxpY6ZNXgNUh+en8nw4epqGWJQCI2kJ0N4bzVIPd6UIBU5ZnSYjho1yjDB9smTJ3n//ffp0KEDkZGRhIaG5nuBJV7NwWBiDjd+551XDmJnB2fOwObNWhcmhEbSk+H+LfW+ZWFcONOnqZ+XdAWOj4K758C6EjT+suA/W5QYRo/NGxkZSd26dQFYvXo1nTp1YtKkSRw7dszQGEkYwdoV3HtA5GJso2fyzjvL+OILmDYNOnXSujghCtmtY7CjJWQkq891Zuqg8ma2YFbmX39ms87EXJ3VJe0upN998Gfik88fPtanPlmH/3ywKFe4310Ua0aHqYWFBcnJ6g/9119/JSQkBIDy5csbzliFkWqPgMjFEL2K0HeimDmzKnv2wOHD0KSJ1sUJUUgetqB9GKSgtqZNi1eXgmZiCRb2UGsEVGpX8J8nShSjw7RFixaEhobSvHlzDh06xPLlywE4d+4cVapUyfcCS4VyPuD8IsTuxCXha3r0+IIlS9Sz0weHV4iS7+TERy1o20eAqfWDM8ikB0viv/587HFaImQ8WKdPfXCmWhbMy6qPzcs+ev5w3b+fm5hrfQREMWZ0a96oqCgGDx5MdHQ0Q4cO5e233wZgxIgRZGRk8NVXXxVIofmlSLXmfdw/G2FPMJjb8afX33g3KouJCVy4AJ6eWhcnRAG7eRi2P/egBe0acOuqdUWiFMjPPJCuMUWFooeNddTGD35f0m7YULZtg6FD4UtpByFKsox7sKURJJwG9zeg+VKtKxKlhOZhmpGRwbp16zh9+jQA9erV4z//+Q+mpqZ5KqYwFNkwBTj/HRweDLae/GpznjZtTbGxgehoKF9e6+KEKCDHR8PpqWDlDB1PgWUFrSsSpYSm/UwvXLhAnTp1CAkJYc2aNaxZs4ZevXpRr149Ll68mKdiSj3PEHVA7aRIXqq1AR8fGWJQlHA3DqhzhQI0/V6CVBRbRofp0KFDqV69OtHR0Rw7doxjx44RFRWFp6cnQ4cOLYgaSw8zW6gxEADd2RmMHKmu/uoruH9fw7qEKAjpKXCgz4MBEnpBlc5aVyRErhkdpnv27GHq1KmUf+y6Y4UKFZgyZQp79uzJ1+JKpRpD1FaF1/fxepvDVKkCsbHw009aFyZEPjsxFhLOqn2t/aRhgCjejA5TS0tL7t69+8T6xMRELCws8qWoUs2mElR9HQCzC4+GGJw2TYYYFCXI9f1wZob6uOlcsJRGAaJ4MzpMO3XqxIABAzh48CCKoqAoCgcOHGDgwIH85z//KYgaS58Hs8kQtZIBvaKxs4PTp2HLFm3LEiJfpCfDgb6AAtX6QGUZ6ksUf0aH6VdffUX16tUJCAjAysoKKysrmjdvjpeXF7NmzSqAEkuh8g3B6QVQ0il79RveeUddPW2aplUJkT/++BjungfrytBoptbVCJEvct3P9MKFC4auMXXq1MHLyytfCysoRbprzOP+3gC/dQZze/5p+jceXmVIT1eHGGzcWOvihMiluL3waytAgRc2Q6X2WlckSjFNu8Y85OXlRXBwMMHBwXh5eXHixAm5Z5qfKneCMl6QFk/le4t4OO/61KnaliVErqUnPbq8W/1tCVJRouQ6TP9NURQyMjLy6+2EzgRqD1cfn53FqJHqsV25Eo4f164sIXItYgwkXgSbKtBwutbVCJGv8i1MRQGo1kedBirxIt7lN9Kjh7p6zBhNqxLCeLG74dzX6mP/+ersLEKUIBKmRZmZLXgNUB+fmcFnn4G5OWzbBjt3aluaEDmWlggH3lIfew0A17ba1iNEAchxmCYkJDx1yarvqcgHNd9VJ0iO+41qDkcZqA6QxOjRULqmKBDFVsRoSIoEm6rQ8AutqxGiQOS4Na+JiQk6nS7b1xVFQafTFfn7psWmNe/j9veEK0vBoydxXv+jenVITIQVK+C117QuToiniAmDnYHq4xd3gEugtvUI8Zj8zIMcTw6+a9euPH2QyIPaI9QwvbIcJ9/PGTmyMhMmwMcfQ5cu6qVfIYqctLtwUJ3vmBqDJEhFiSbzmRYXO56H63uh7mjuVp9C9epw/Tp89x2GS79CFCmHBsKF78HWAzqcBPMyWlckRCZFop+pKGS1Q9U/z8ygbMo+xo5Vn06cCElJ2pUlRJaubVeDFOC5BRKkosTTPExnz56Nh4cHVlZW+Pv7c+jQoaduf+fOHYYMGYKrqyuWlpbUrFmTzZs3F1K1GqrSGap2A30a7HuVd978G09PiIkBGcVRFCmp8XCwn/q45rvg3FrbeoQoBJqG6fLlywkNDWX8+PEcO3YMHx8fgoKCiIuLy3L71NRU2rRpw+XLl1m1ahVnz55l3rx5VK5cuZAr14BOp/4L38Eb7sViceBlJv3fPUAdFenGDY3rEwJAnwGHBkByNJSpBr5TtK5IiEKhaZjOmDGD/v3707dvX+rWrcucOXOwsbFhwYIFWW6/YMECbt26xbp162jevDkeHh60atUKHx+fQq5cI2a28Pw6dSCHW4fp7jkIX1+FhASYNEnr4kSpp89QJ/uOWgE6U3huofqbFaIU0CxMU1NTOXr0KIGBj1r4mZiYEBgYSHh4eJb7bNiwgYCAAIYMGYKzszP169dn0qRJT+2Oc//+/Sf6xBZrZapB8+WgM0EXuYil42cDMHs2XLmicW2i9HoYpJf/pwZp8+Xg9LzWVQlRaHLUNebll1/O8RuuWbMmR9vduHGDjIwMnJ2dM613dnbmzJkzWe5z6dIldu7cSc+ePdm8eTMXLlxg8ODBpKWlMX78+Cz3mTx5MhMnTsxx/cWCaxvwnQrHR1I7ZTjDe3gza2krxo+HRYu0Lk6UOlkFadVXtK5KiEKVozNTe3t7w2JnZ0dYWBhHjhwxvH706FHCwsKwty/Y8Tb1ej1OTk7MnTsXPz8/unfvzscff8ycOXOy3WfMmDHEx8cblujo6AKtsdDUDgX3HuiUDKb+5zXcKkSxZAmcPKl1YaJUkSAVAsjhmenChQsNj0ePHk23bt2YM2cOpqamAGRkZDB48GCj+uk4OjpiampKbGxspvWxsbG4uLhkuY+rqyvm5uaGzwV1LtWYmBhSU1OznALO0tISS0vLHNdVbOh04D8PEv7C/HYEuz/tSr3h+/joI2t++UXr4kSpoM+AA73h8k/qkJfNl0mQilLL6HumCxYsYOTIkZkCzdTUlNDQ0GwbDmXFwsICPz8/wsLCDOv0ej1hYWEEBARkuU/z5s25cOECer3esO7cuXO4urqWzrlUzWzUBkmWjlRzOMa8/gPYuFFh716tCxMlngSpEJkYHabp6elZ3tM8c+ZMppDLidDQUObNm8fixYs5ffo0gwYNIikpib59+wIQEhLCmMfmGxs0aBC3bt1i2LBhnDt3jk2bNjFp0iSGDBli7NcoOWzdoYXaerJX8/8xrN2XMgi+KFgSpEI8Icdj8z7Ut29f3n77bS5evEjTpk0BOHjwIFOmTDGEYE51796d69evM27cOGJiYvD19WXr1q2GRklRUVGYmDzKezc3N7Zt28aIESNo0KABlStXZtiwYYwePdrYr1GyOLdWJ1s+NpxpPUYS9Lk369e/RJcuWhcmShwJUiGyZPTYvHq9nmnTpvHll19y7do1QL2XOWzYMN5///1Ml3+LomI7Nu+zKIraECRyCTfuVuD1RUfYutcDM6P/uSRENvQZEB6iTrqgM4MWy8Et5y39hShq8jMP8jTQ/cM+m8UplEpsmAKkp5C+7XnM4o8QccWHCKff6fO2jdZViZJAglSUQJoPdJ+ens6vv/7Kzz//bJjj9OrVqyQmJuapGJFHZtaYvbCGpAwnfN3/wOHs26Qky81TkUf6dAlSIZ7B6DC9cuUK3t7edO7cmSFDhnD9+nUAPv/8c0aOHJnvBQoj2bph1noVaRlmdPFdxsFF07SuSBRn+nQI7y1BKsQzGB2mw4YNo3Hjxty+fRtra2vD+q5du2bq5iK0Y1mlJUf5EoCWdh9y99x2jSsSxdITQbpCglSIbBgdpnv37uWTTz55ol+nh4cH//zzT74VJvKmyRuDWB3xNqYmekwPvA53L2pdkihOsgzSrlpXJUSRZXSY6vX6LAeW//vvvylbtmy+FCXyztRMh3XL2Ry44I+N2W3SwrpAmtzTFjmQ/A/83lOCVAgjGB2mbdu2ZdZjs1HrdDoSExMZP348HTp0yM/aRB6172TJ57+v5tptF8yT/1S7zmTc17osURSlp8Dln2FnEKxzezCNmgSpEDlldNeYv//+m6CgIBRF4fz58zRu3Jjz58/j6OjIb7/9hpOTU0HVmi9KdNeYLPz+O4zs8zu7P3kBC7M0sKwIXgOgxkCwqaJ1eUJLigI3DkDkIriyHNLiH73m9DzUHwsugdnuLkRxp3k/0/T0dJYvX84ff/xBYmIijRo1omfPnpkaJBVVpS1MAbp0AdOrq/l+wHAcbf5WV+pMoUpXqPUeVGypDpwvSofkvyHyR7i0CO6ee7Te1h08e4NnCJStrll5QhQWzcO0OCuNYfrXX+DtDTrSObZhPQ0sv4a4PY82cGgANd8Fjx5gZqtdoaLgpKfA3+vUAI3ZATz4z97UBqq+CtX6gFMr0OWq67kQxZKmYWpqasrzzz/P6tWrKV++vGF9bGwslSpVyrJxUlFSGsMUYPBg+O47cHGBP/4AJ4uTcO4b9QwlI0XdyNwBqr8NNQdDmWqa1ivywbMu43r2UYPUXBoOitJJ0zA1MTHhueeeIyYmhl9++YV69eoBapi6uroaPXNMYSutYZqUBE2bqmepbdvCli1gYgKk3oaLC+H8bEi89GBrHVTqqF4CdgmUs5Xi5t4NuLQQLv4gl3GFeApNhxPU6XSsXr2a4OBgAgICWL9+fabXRNFkawsrVoC1NWzfDp9//uAFi3JQJxQ6nYNWG8E1CFDg6kbYFQSb6sLZryEtQcvyxbMoCtw4qPYNXVcFIj5Qg9TURg3Pl3bCfy5Bg4kSpEIUgFydmcbExODk5MTcuXMZOnQon3zyCf369aNy5cpymbeIW7AA3n4bTE1h925o0SKLjRLOwrlv1bOb9LvqOrMyUHc01PtIzlSLkvRkuPKz+vd1+9ij9eUaQY1B4N5dLuMKkQ3NL/M+DFOAXbt28dprr9GoUSPCwsIkTIs4RYE334SffoIqVSAiAipUyGbjtLsQuUS9t5rwYEL4Sh2g2f/UM1qhnYSzcP47tUHRw3uhJpbg/jrUGAwVmkgLbSGeQdMw9fT05MiRI1R47P/AFy5cIDg4mHPnzkmYFgN374KfH5w/D506wYYNz/j/rqJA5GI4PAgy7kGZ6vD8WnDwLrSaBeoQf/9sUM9CYx8bB7tMNfUstFpfsMzuX0ZCiH8rkl1j7t27R2xsLO7u7vnxdgVGwlQVEQHPPQf378OMGTBiRA52unUc9r4MSZfVe3HPLVAvI4qClXxVbUx0YS6kPBz/WgeVO6lnoa5t5dK7ELlQJMO0uJAwfeTbb2HIEDA3h3371Na+z3T/Jux/40FfRaD2++A7BUzMCrTWUkdR1L7A57+F6LWgpKvrLStC9X7qKFZlPDQtUYjirtDDtHz58pw7dw5HR0fKlSv31Fa7t27dylNBBU3C9BFFgW7dYNUq8PCA48fBwSEHO+oz4MQn8NcU9blza2i+HKwqFmC1pURaAlxaooZowulH6yu2UM9C3V4GU0vt6hOiBMnPPMjR6cTMmTMNM8I8Psi9KN50Opg3D44ehchI6NcPVq7MQbsVE1PwnQzlG6uD58fugq1+0HINVGhcGKWXPHdOqvdCL/8I6UnqOjNb8Oilhmi5BtrWJ4R4KrnMKzh8GJo3h7Q0mD1bHS0px+L/gt+6qn0aTSyhybdQ/a0Cq7VEyUiF6DXqgBnX9z1ab1dHDdBqIWAuv1EhCkqhX+ZNSMh5h/2iHlASplmbORNCQ8HCAg4eBF9fI3ZOjYfwELWlKYDXQPD7Ekwtnr5fUacocOcP0JlDWa/8u7yaFA0XvoeL8+BenLru4cQDNYc8GCNXurUIUdAKPUxNTEyeObqRoijodDrpGlNMKQp07gy//AI1aqiXfo2a613Rw6lJcGIcoIBjALRYBTaV8lZYRqo6zKFNpcI7S0uJUfvXXpz/aDg+nQnYVgO72mBfR/3z4WJZ/unvB+rxid0J52ar/+hQHgy7ae0KXu9A9f55P1ZCCKMUepju2bPnWZsYtGrVKk8FFTQJ0+zduqWekUZHQ48e8L//5eIE6Z/N8HtPSLsDVs5qoDplNczSvyh6SIyE+D/V+4d3/lQfJ5xVW7KaWIDzS2oDnCr/Aat8njdXnw7XtqpdUP7ZCMqDfxSa2qgtlZ82nKJlxczhalcb7GuDjbs6gtSlReoAC4+Pk+vcWr2UW6UzmJjn73cRQuSIdI3JAwnTp9u/H1q1gowM+OEHdehBo929oPZHvXMSdGbQaKZ6+VKnU0+B78Wqr2UKzlOQkZz1+5laP5rZBtSzxIot1cuibl3Btmquvquh1osL1JlVUq49Wu8YoM6gU7WbOpTivRh1FKiEMxB/+tHj5Ojs39vUSv0z4576p1lZqNZbHWDBvm7uaxZC5IsiEabJyclERUWRmpqaaX2DBkW71aGE6bNNmQJjxqiD4h86BPXr5+JN0pPgYD+4skx97tIW9PfVAL1/M+t9TCzVS6j23uBQXx1hyb4+2FRRgyt6Dfy9Fm4dzbxf+cZqqFZ5WT0jfGZtKRC9Wj0LfXxeV0tHdVD46m/nPOzSEtUzzofh+jBs754D/YP/Nhy81bNQj15gXiZn7yuEKHCahun169fp27cvW7ZsyfJ1uWda/On10L69OrtM3bpqoNrmZs5wRYGzs+D4qEeXTUE9syzjpQbm48FZpnrOBn9IuqIOZBC95kEr2Md+wnZ11EvBbl3Vwd4fXqdWFHUg+Ivz4fLSR+PZ6kzAJUgN0MrB+ddoSp+hjhSVkQL29aRBkRBFkKZh2rNnT65cucKsWbN44YUXWLt2LbGxsfzf//0f06dPp2PHjnkqqKBJmOZMXBz4+EBMDLz1Fsyfn4c3u3EArm5Vx5B1qK8Gnpl1/hSaEqs26Ileo45Xq0979JpN1QeXgd3VBkW3Ix69ZusB1d6Can3A1i1/ahFCFCuahqmrqyvr16+nadOm2NnZceTIEWrWrMmGDRuYOnUq+/bte/abaEjCNOd27YLAQPVM9ccfoVcvrSt6htR4uLpJDdarW568B2tiqZ61Vn9bbQAk49kKUaoV+ghIj0tKSjJMv1auXDmuX79OzZo18fb25tixY8/YWxQnrVvDuHEwYQIMHAhNmkCtWlpX9RQW9uDRQ13Sk9Xxg6NWq4PDV+kCHj1z1o1FCCGMZHSY1qpVi7Nnz+Lh4YGPjw/ff/89Hh4ezJkzB1dX14KoUWjok0/UScR371b7oe7ZA87OWleVA2Y2areTKp21rkQIUQoYHabDhg3j2jW1C8H48eNp164dP/30ExYWFixatCi/6xMaMzWFpUvB3x/OnlUv++7aBY6OWlcmhBBFh9E3jXr16kWfPn0A8PPz48qVKxw+fJjo6Gi6d8/d3JazZ8/Gw8MDKysr/P39OXToUI72W7ZsGTqdji5duuTqc0XOuLrCzp3qn3/+CW3bwu3bWlclhBBFR55bYNjY2NCoUSMcc3mqsnz5ckJDQxk/fjzHjh3Dx8eHoKAg4uLinrrf5cuXGTlyJC1btszV5wrjeHmpgerkpE7V1q4dGDFksxBClGhGt+ZVFIVVq1axa9cu4uLi0Ov1mV5fs2aNUQX4+/vTpEkTvvnmGwD0ej1ubm689957fPjhh1nuk5GRwfPPP89bb73F3r17uXPnDuvWrcvR50lr3rw5eVJtmHTzpjrTzNatUEbGIRBCFEP5mQdGn5kOHz6cN998k8jISMqUKYO9vX2mxRipqakcPXqUwMDARwWZmBAYGEh4eHi2+3366ac4OTnxdg7Gurt//z4JCQmZFpF73t7qYA4ODurQg8HBkJzNKIBCCFFaGN0A6ccff2TNmjV06NAhzx9+48YNMjIycP5X81BnZ2fOnDmT5T779u1j/vz5RERE5OgzJk+ezMSJE/NaqnhMo0awbZvaGGn3bujaFdavBysrrSsTQghtGH1mam9vT7Vq1Qqilme6e/cub775JvPmzcvxPdoxY8YQHx9vWKKjnzIwucixpk1hyxZ1mMHt2+HVV+FfwzQLIUSpYXSYTpgwgYkTJ5KSkvLsjZ/B0dERU1NTYmNjM62PjY3FxcXlie0vXrzI5cuXCQ4OxszMDDMzM5YsWcKGDRswMzPj4sWLT+xjaWmJnZ1dpkXkj+bN1flPraxg0yZ4/XVIS3v2fkIIUdIYHabdunXj9u3bODk54e3tTaNGjTItxrCwsMDPz4+wsDDDOr1eT1hYGAEBAU9sX7t2bU6ePElERIRh+c9//kPr1q2JiIjAzU3GWC1srVurl3gtLGDtWggJUadvE0KI0sToe6a9e/fm6NGj9OrVC2dnZ3R5nA0jNDSU3r1707hxY5o2bcqsWbNISkqib9++AISEhFC5cmUmT56MlZUV9f81H5iDgwPAE+tF4WnbFlavhpdfhmXL1GBduBBMZOhbIUQpYXSYbtq0iW3bttGiRYt8KaB79+5cv36dcePGERMTg6+vL1u3bjU0SoqKisJE/q9c5HXqpAZpt26wZAlYWsKcORKoQojSweh+prVr12bFihVFfhLw7Eg/04K1bBn07KnONPPuu/DVVzKVpxCiaNK0n+n06dP54IMPuHz5cp4+WJRMr7+uXuLV6eCbb2DUKHVebiGEKMmMvszbq1cvkpOTqV69OjY2Npibm2d6/datW/lWnCieQkLg/n0YMACmT1db+/7f/2ldlRBCFByjw3TWrFkFUIYoafr3VwP1vffgv/9V76GOHat1VUIIUTCMCtO0tDT27NnD2LFj8fT0LKiaRAnx7rtqoI4cqU4ynpwMkybJPVQhRMlj1D1Tc3NzVq9eXVC1iBLo/fdhyhT18ZQp0Lu3jJQkhCh5jG6A1KVLlxzP0CIEwOjRsGCBOtH4jz+q3Wju3tW6KiGEyD9G3zOtUaMGn376Kfv378fPzw9bW9tMrw8dOjTfihMlR9++4OKijuG7Ywe0agWbN6vrhBCiuDO6n+nT7pXqdDouXbqU56IKkvQz1dbhw9CxI1y/Dh4e6nyotWppXZUQojTKzzww+sw0MjIyTx8oSrcmTSA8HNq1gwsXHg2Wn8VQzEIIUWzkabA3RVEw8sRWCKpXVycWb9IEbt6EF1+EDRu0rkoIIXIvV2G6ZMkSvL29sba2xtramgYNGvDjjz/md22iBHNygl27oEMHuHdPnWD8+++1rkoIIXLH6DCdMWMGgwYNokOHDqxYsYIVK1bQrl07Bg4cyMyZMwuiRlFC2dqq07e9/bY6lu/AgerADnKxQwhR3OSqAdLEiRMJCQnJtH7x4sVMmDChyN9TlQZIRY+iwMSJ6gJqy9/vv4d/jVQphBD5StOB7q9du0azZs2eWN+sWTOuXbuWp2JE6aTTwYQJMHeuOmXbwoXQuTMkJmpdmRBC5IzRYerl5cWKFSueWL98+XJq1KiRL0WJ0ql/f1i3DqytYcsWaN0a4uK0rkoIIZ7N6K4xEydOpHv37vz22280b94cgP379xMWFpZlyAphjOBgtWFSx45w5Ag0a6b2RfXy0royIYTIntFnpq+88goHDx7E0dGRdevWsW7dOhwdHTl06BBdu3YtiBpFKePvD7//Dp6ecPGiGqgHDmhdlRBCZM/oBkjFnTRAKj5iY9WuM8eOgZmZ2kBp9Gh1jF8hhMgrTRsgCVFYnJ1h9251PN/0dPj4Y3VM3yLeYFwIUQrlOExNTEwwNTV96mJmZvQtWCGeqmxZWLECFi9WH+/fDz4+6vPSdU1FCFGU5fgy7/r167N9LTw8nK+++gq9Xs+9e/fyrbiCIJd5i6/ISHjzTTVQQT1jnTMHKlTQti4hRPGUn3mQp3umZ8+e5cMPP+SXX36hZ8+efPrpp7i7u+epoIImYVq8ZWTA55/D+PHqpV9XV1i0CNq21boyIURxo/k906tXr9K/f3+8vb1JT08nIiKCxYsXF/kgFcWfqSl89JHaurdWLbh2DYKCYPhwSEnRujohRGllVJjGx8czevRovLy8OHXqFGFhYfzyyy/Ur1+/oOoTIkt+fmor38GD1edffqnOQvPHH9rWJYQonXIcplOnTqVatWps3LiRn3/+md9//52WLVsWZG1CPJWNDcyeDZs2qS1/T51SA/WLL9TLwUIIUVhyfM/UxMQEa2trAgMDMX1KR781a9bkW3EFQe6ZlkzXr6vDET5sJ/fCC2qL36pVNS1LCFGE5Wce5LgvS0hICDqdLk8fJkRBqVgR1q6F+fPV+6e7d0ODBvDtt9Cjh9bVCSFKOhkBSZQ4Fy5Ar15w8KD6/PXXYcYMteWvEEI8pHlrXiGKMi8v2LdPndbN1BSWLYMaNdThCGVaNyFEQZAwFSWSmZnaF/X336FpU0hKUsO1Rg2YN0/toyqEEPlFwlSUaE2bqn1Sly1TZ6GJiYEBA8DXV20FXLpucgghCorcM81GRkYGaWlphViZyA/m5ubZtja/f19tkPTZZ3D7trqudWuYNg0aNSrEIoUQRUKRGU4wv8yePZsvvviCmJgYfHx8+Prrr2natGmW286bN48lS5bw559/AuDn58ekSZOy3f7fnnXwFEUhJiaGO3fu5Pr7CG05ODjg4uKSbevz27dh0iT46itITVXX9eoF//2vdKURojQpUWG6fPlyQkJCmDNnDv7+/syaNYuVK1dy9uxZnJycnti+Z8+eNG/enGbNmmFlZcXnn3/O2rVrOXXqFJUrV37m5z3r4F27do07d+7g5OSEjY2NdAcqRhRFITk5mbi4OBwcHHB9RvPdy5fVad2WLlWfW1rCsGEwZgw4OBR4uUIIjZWoMPX396dJkyZ88803AOj1etzc3Hjvvff48MMPn7l/RkYG5cqV45tvviEkJOSZ2z/t4GVkZHDu3DmcnJyoIFORFFs3b94kLi6OmjVrPnWAkYeOHIFRo9S+qaDOQjN2LAwaBBYWBVurEEI7JaZrTGpqKkePHiUwMNCwzsTEhMDAQMLDw3P0HsnJyaSlpVG+fPksX79//z4JCQmZluw8vEdqY2NjxLcQRc3Dv7+c3vNu3Bh27oRffoE6deDmTXXgh7p1YdUqaaQkhHg2TcP0xo0bZGRk4OzsnGm9s7MzMTExOXqP0aNHU6lSpUyB/LjJkydjb29vWNzc3J75nnJpt3jLzd+fTgedOsGJE/D99+pYvxcvwmuvgb8/bN4soSqEyF6x7hozZcoUli1bxtq1a7GysspymzFjxhAfH29YoqOjC7lKUZyYmaldZy5cUPup2tjA4cPQsSM89xxs2SKhKoR4kqZh6ujoiKmpKbGxsZnWx8bG4uLi8tR9p02bxpQpU9i+fTsNGjTIdjtLS0vs7OwyLeLZPDw8mDVrlubvoZUyZdRBHiIjYeRIsLaGQ4egQwcICICtWyVUhRCPaBqmFhYW+Pn5ERYWZlin1+sJCwsjICAg2/2mTp3KZ599xtatW2ncuHFhlFrkvfDCCwwfPjzf3u/w4cMMGDAg396vuHJyUqd0i4yE999XQ/XgQWjfHpo1g23bJFSFEEXgMm9oaCjz5s1j8eLFnD59mkGDBpGUlETfvn0BdbaaMWPGGLb//PPPGTt2LAsWLMDDw4OYmBhiYmJIlEFXn0lRFNJzOI5exYoVpSHWY5yd1cEdIiMhNFQN1QMHoF07aN4ctm+XUBWiNNM8TLt37860adMYN24cvr6+REREsHXrVkOjpKioKK5du2bY/rvvviM1NZVXX30VV1dXwzJt2rQCqU9R1HFdtVhy+j/nPn36sGfPHr788kt0Oh06nY7Lly+ze/dudDodW7Zswc/PD0tLS/bt28fFixfp3Lkzzs7OlClThiZNmvDrr79mes9/X6LV6XT88MMPdO3aFRsbG2rUqMGGDRuMOpZRUVF07tyZMmXKYGdnR7du3TJd4v/jjz9o3bo1ZcuWxc7ODj8/P44cOQLAlStXCA4Oply5ctja2lKvXj02b95s1OfnB2dnmD4dLl1SQ9XKCsLDISgIWrSAHTskVIUolZRSJj4+XgGU+Pj4J15LSUlR/vrrLyUlJcWwLjFRUdT/PRb+kpiYs+90584dJSAgQOnfv79y7do15dq1a0p6erqya9cuBVAaNGigbN++Xblw4YJy8+ZNJSIiQpkzZ45y8uRJ5dy5c8onn3yiWFlZKVeuXDG8p7u7uzJz5kzDc0CpUqWKsnTpUuX8+fPK0KFDlTJlyig3b97Mtq7H3yMjI0Px9fVVWrRooRw5ckQ5cOCA4ufnp7Rq1cqwfb169ZRevXopp0+fVs6dO6esWLFCiYiIUBRFUTp27Ki0adNGOXHihHLx4kXll19+Ufbs2ZPl52b191hQrl1TlBEjFMXK6tHfW7NmirJ9u6Lo9QX+8UKIPHhaHhhLwvQxxTVMFUVRWrVqpQwbNizTuodhum7dumfuX69ePeXrr782PM8qTD/55JPHjkuiAihbtmzJ9j0ff4/t27crpqamSlRUlOH1U6dOKYBy6NAhRVEUpWzZssqiRYuyfC9vb29lwoQJz/weilK4YfrQ1auKMnx45lBt3lxCVYiiLD/DVPPLvEWdjY06B6YWS37dsvx3I63ExERGjhxJnTp1cHBwoEyZMpw+fZqoqKinvs/jraZtbW2xs7MjLi4uRzWcPn0aNze3TP1869ati4ODA6dPnwbU++f9+vUjMDCQKVOmcPHiRcO2Q4cO5f/+7/9o3rw548eP58SJEzn63MLi6gozZ6qXf4cNU4cm3L8f2rZVB4KYPh1u3NC6SiFEQZEwfQadDmxttVnya+wIW1vbTM9HjhzJ2rVrmTRpEnv37iUiIgJvb29SH476ng1zc/N/HRsder0+f4oEJkyYwKlTp+jYsSM7d+6kbt26rF27FoB+/fpx6dIl3nzzTU6ePEnjxo35+uuv8+2z84urK8yapYbq0KHq3+PZs2r3msqV4fXX1dGW8vGwCSGKAAnTEsLCwoKMjIwcbbt//3769OlD165d8fb2xsXFhcuXLxdofXXq1CE6OjrToBl//fUXd+7coW7duoZ1NWvWZMSIEWzfvp2XX36ZhQsXGl5zc3Nj4MCBrFmzhvfff5958+YVaM15UakSfPklXLsGc+eqQxampsLy5fDSS1CzJnz+Ofyri7UQopiSMC0hPDw8OHjwIJcvX+bGjRtPPWOsUaMGa9asISIigj/++IMePXrk6xlmVgIDA/H29qZnz54cO3aMQ4cOERISQqtWrWjcuDEpKSm8++677N69mytXrrB//34OHz5MnTp1ABg+fDjbtm0jMjKSY8eOsWvXLsNrRVnZstC/vzqK0rFj6uD5ZcuqQxV++CFUqQKvvqp2rZGzVSGKLwnTEmLkyJGYmppSt25dKlas+NT7nzNmzKBcuXI0a9aM4OBggoKCaFTAs2PrdDrWr19PuXLleP755wkMDKRatWosX74cAFNTU27evElISAg1a9akW7dutG/fnokTJwLqjD5DhgyhTp06tGvXjpo1a/Ltt98WaM35rWFDdXLya9dg/nx1eML0dFi9Wu1aU726Oqfq1ataVyqEMJbmU7AVtqdNuXPv3j0iIyPx9PTMdqxfUfQVp7/HEydg3jz48UeIj1fXmZqqg+4PGKCGbA5mkRNC5EKJmYJNiNKuQQP4+mv1bHTxYnXgh4wMWL9eHVy/ShV47z3Yt08uAwtRlEmYClEE2NhASAjs3QunTqnzqZYvDzEx8M030LIlVK0KI0aowxiWrutJQhR9EqZCFDF166p9Vq9dg02b1JC1s4N//lG73QQEgKcnfPABHD0qwSpEUSBhKkQRZWGhTvm2eLHahWbdOnjjDbXv6pUr6mw2jRur3Ww+/li9/yrBKoQ2JEyFKAasrKBzZ1i6FOLiYOVKtUuNtbU6kfmkSeDjo57VTpgADwaVEkIUEglTIYoZGxs1SFeuVIN16VI1aC0s4MwZmDhRDVUfH/VysQxjKETBkzAVohgrU0a99LtunRqsixerl4bNzNTLvqGh6mhMr70GW7eqLYWFEPlPwlSIEsLeXm2stGmTeo/122/Bzw/S0mDVKmjfXm24NG6cOsm5ECL/SJgKUQKVL68OXXjkCEREqH1Vy5WD6Gj47DOoVk0dI3jpUrh3T+tqhSj+JEyFgYeHB7Nmzcr29T59+tClS5dCq0fkDx8f+OordWCIn3+GNm3UGYl27oSePdWZbt59F44f17pSIYovCVMhSgkrK3UKuO3b1Snixo9XB4K4cwdmz4ZGjdRl9my4fVvraoUoXiRMhSiFPDzULjSXLsG2bdCtm9oa+Phx9Sy1UiXo2lUdN/jvv7WuVoiiT8L0WRQF0pO0WXLYA3/u3LlUqlTpiWnUOnfuzFtvvQXAxYsX6dy5M87OzpQpU4YmTZrw66+/5unQ3L9/n6FDh+Lk5ISVlRUtWrTg8OHDhtdv375Nz549qVixItbW1tSoUcMwP2lqairvvvsurq6uWFlZ4e7uzuTJk/NUjzCeqSm0bavOs3r1qjrCkre3eh913Tp1sH03N/VS8Zgx6nCH6elaVy1E0WOmdQFFXkYyrCijzWd3SwQz22du9tprr/Hee++xa9cuXnrpJQBu3brF1q1b2bx5MwCJiYl06NCB//73v1haWrJkyRKCg4M5e/YsVatWzVV5H3zwAatXr2bx4sW4u7szdepUgoKCuHDhAuXLl2fs2LH89ddfbNmyBUdHRy5cuEBKSgoAX331FRs2bGDFihVUrVr1iYnDReGrUAGGDYOhQ9VGSxs3wubNcPCg2s3mxAmYMgUcHNQA7tAB2rUDZ2etKxdCexKmJUC5cuVo3749S5cuNYTpqlWrcHR0pHXr1gD4+Pjg4+Nj2Oezzz5j7dq1bNiwgXfffdfoz0xKSuK7775j0aJFtG/fHoB58+axY8cO5s+fz6hRo4iKiqJhw4Y0btwYUBs4PRQVFUWNGjVo0aIFOp0Od3f33H59kc90OnXu1YYNYexYddCH7dvVYN26FW7ehBUr1AXUIQ07dFCXxo1lyjhROkmYPoupjXqGqNVn51DPnj3p378/3377LZaWlvz000+8/vrrmJioV/ITExOZMGECmzZt4tq1a6Snp5OSkvLUScSf5uLFi6SlpdG8eXPDOnNzc5o2bcrpB2PZDRo0iFdeeYVjx47Rtm1bunTpQrNmzQC1ZXCbNm2oVasW7dq1o1OnTrRt2zZXtYiC5egIPXqoS0YGHD6sBuvmzepA+0eOqMunn6pnt+3awYsvqq2EK1RQ969QQR2sX6fT+tsIUTAkTJ9Fp8vRpVatBQcHoygKmzZtokmTJuzdu5eZM2caXh85ciQ7duxg2rRpeHl5YW1tzauvvkpqamqB1dS+fXuuXLnC5s2b2bFjBy+99BJDhgxh2rRpNGrUiMjISLZs2cKvv/5Kt27dCAwMZNWqVQVWj8g7U1N47jl1+fRTdYq4rVvVYN2+XT1r/ekndfk3MzO1/+vjAftwefy5oyPUqAFOToX//YTILQnTEsLKyoqXX36Zn376iQsXLlCrVi0aNWpkeH3//v306dOHrl27AuqZ6uXLl3P9edWrV8fCwoL9+/cbLtGmpaVx+PBhhg8fbtiuYsWK9O7dm969e9OyZUtGjRrFtGnTALCzs6N79+50796dV199lXbt2nHr1i3Kly+f67pE4XJxgT591CUtDcLD1WA9ckQN1odLcrLacCkuTl1yws1NvWzs5/foT0fHgvw2QuSehGkJ0rNnTzp16sSpU6fo1atXptdq1KjBmjVrCA4ORqfTMXbs2Cda/xrD1taWQYMGMWrUKMqXL0/VqlWZOnUqycnJvP322wCMGzcOPz8/6tWrx/3799m4cSN16tQBYMaMGbi6utKwYUNMTExYuXIlLi4uODg45LomoS1zc3j+eXX5t5SUzOF640b2z+Pi1CnmoqPVZe3aR+/j7q6G6uMBW6FC4X1HIbIjYVqCvPjii5QvX56zZ8/So0ePTK/NmDGDt956i2bNmuHo6Mjo0aNJSEjI0+dNmTIFvV7Pm2++yd27d2ncuDHbtm2jXLlyAFhYWDBmzBguX76MtbU1LVu2ZNmyZQCULVuWqVOncv78eUxNTWnSpAmbN2823OMVJYu1NVSpoi45kZCg9nl9eE/26FE4d04N2StXYM2aR9t6eGQO10aNJGBF4dMpSumaTjghIQF7e3vi4+Oxs7PL9Nq9e/eIjIzE09MTKysrjSoUeSV/jyVTfPyTAXv+fNbbVqgAtWqpE6c//LNmTfDyUkeCEgKengfGkjNTIUSxYG8PL7ygLg/Fx8OxY5kD9sIF9XLx77+ry+N0OvVS8eMh+/BPNzeQCyMityRMhRDFlr09tG6tLg8lJalnrOfOqcvZs4/+jI+Hy5fVZfv2zO9lZaWeubq6qgNT2Ntn/jO7x2XKSAgLCVMhRAljawu+vuryOEWB69efDNhz59Sz2Xv34M8/1cUYJiZqH9qHITtlCgQF5c93EcVHkQjT2bNn88UXXxATE4OPjw9ff/01TZs2zXb7lStXMnbsWC5fvkyNGjX4/PPP6dChQyFWLIQobnQ6te+qkxO0aJH5tfR0tWHTuXNqy+L4eHU2nTt3Hj3Oal1qKuj1j9YD3L9feN9JFB2ah+ny5csJDQ1lzpw5+Pv7M2vWLIKCgjh79ixOWfTa/v3333njjTeYPHkynTp1YunSpXTp0oVjx45Rv379fKmplLXJKnHk708Yy8wMqldXF2Pcu/dkwDZsWAAFiiJP89a8/v7+NGnShG+++QYAvV6Pm5sb7733Hh9++OET23fv3p2kpCQ2btxoWPfcc8/h6+vLnDlznvl5T2u9lZGRwblz53BycqKCtK0vtm7evElcXBw1a9bEVAaKFUJko8S05k1NTeXo0aOMGTPGsM7ExITAwEDCw8Oz3Cc8PJzQ0NBM64KCgli3bl2W29+/f5/7j113eVrfSlNTUxwcHIh7MESLjY0NOhlMtNhQFIXk5GTi4uJwcHCQIBVCFBpNw/TGjRtkZGTg/K85nJydnTlz5kyW+8TExGS5fUxMTJbbT548mYkTJ+a4JhcXFwBDoIrix8HBwfD3KIQQhUHze6YFbcyYMZnOZBMSEnBzc8t2e51Oh6urK05OTqSlpRVGiSIfmZubyxmpEKLQaRqmjo6OmJqaEhsbm2l9bGxstmcWLi4uRm1vaWmJpaWl0bWZmprK/5SFEELkiKZdjS0sLPDz8yMsLMywTq/XExYWRkBAQJb7BAQEZNoeYMeOHdluL4QQQhQ0zS/zhoaG0rt3bxo3bkzTpk2ZNWsWSUlJ9O3bF4CQkBAqV67M5MmTARg2bBitWrVi+vTpdOzYkWXLlnHkyBHmzp2r5dcQQghRimkept27d+f69euMGzeOmJgYfH192bp1q6GRUVRUVKaZRJo1a8bSpUv55JNP+Oijj6hRowbr1q3Ltz6mQgghhLE072da2OLj43FwcCA6OjrP/YqEEEIUXw8bpN65cwd7e/s8vZfmZ6aF7ebNmwBPbdErhBCi9Lh586aEqbHKly8PqJeP83rwSpOH/4KTM/qck2OWO3LcjCfHLHfi4+OpWrWqIRfyotSF6cP7r/b29vKjywU7Ozs5bkaSY5Y7ctyMJ8csd0zyYQ49mYVPCCGEyCMJUyGEECKPSl2YWlpaMn78+FyNilSayXEznhyz3JHjZjw5ZrmTn8et1HWNEUIIIfJbqTszFUIIIfKbhKkQQgiRRxKmQgghRB5JmAohhBB5VOrCdPbs2Xh4eGBlZYW/vz+HDh3SuqQia8KECeh0ukxL7dq1tS6ryPntt98IDg6mUqVK6HQ61q1bl+l1RVEYN24crq6uWFtbExgYyPnz57Uptgh51nHr06fPE7+/du3aaVNsETF58mSaNGlC2bJlcXJyokuXLpw9ezbTNvfu3WPIkCFUqFCBMmXK8MorrzwxB3RpkpNj9sILLzzxWxs4cKBRn1OqwnT58uWEhoYyfvx4jh07ho+PD0FBQcTFxWldWpFVr149rl27Zlj27dundUlFTlJSEj4+PsyePTvL16dOncpXX33FnDlzOHjwILa2tgQFBXHv3r1CrrRoedZxA2jXrl2m39/PP/9ciBUWPXv27GHIkCEcOHCAHTt2kJaWRtu2bUlKSjJsM2LECH755RdWrlzJnj17uHr1Ki+//LKGVWsrJ8cMoH///pl+a1OnTjXug5RSpGnTpsqQIUMMzzMyMpRKlSopkydP1rCqomv8+PGKj4+P1mUUK4Cydu1aw3O9Xq+4uLgoX3zxhWHdnTt3FEtLS+Xnn3/WoMKi6d/HTVEUpXfv3krnzp01qae4iIuLUwBlz549iqKovy1zc3Nl5cqVhm1Onz6tAEp4eLhWZRYp/z5miqIorVq1UoYNG5an9y01Z6apqakcPXqUwMBAwzoTExMCAwMJDw/XsLKi7fz581SqVIlq1arRs2dPoqKitC6pWImMjCQmJibT787e3h5/f3/53eXA7t27cXJyolatWgwaNMgw65NQxcfHA48m8Dh69ChpaWmZfm+1a9ematWq8nt74N/H7KGffvoJR0dH6tevz5gxY0hOTjbqfUvNQPc3btwgIyPDMOn4Q87Ozpw5c0ajqoo2f39/Fi1aRK1atbh27RoTJ06kZcuW/Pnnn5QtW1br8oqFmJgYgCx/dw9fE1lr164dL7/8Mp6enly8eJGPPvqI9u3bEx4ejqmpqdblaU6v1zN8+HCaN29O/fr1AfX3ZmFhgYODQ6Zt5femyuqYAfTo0QN3d3cqVarEiRMnGD16NGfPnmXNmjU5fu9SE6bCeO3btzc8btCgAf7+/ri7u7NixQrefvttDSsTpcHrr79ueOzt7U2DBg2oXr06u3fv5qWXXtKwsqJhyJAh/Pnnn9KOwQjZHbMBAwYYHnt7e+Pq6spLL73ExYsXqV69eo7eu9Rc5nV0dMTU1PSJVm2xsbG4uLhoVFXx4uDgQM2aNblw4YLWpRQbD39b8rvLu2rVquHo6Ci/P+Ddd99l48aN7Nq1iypVqhjWu7i4kJqayp07dzJtL7+37I9ZVvz9/QGM+q2VmjC1sLDAz8+PsLAwwzq9Xk9YWBgBAQEaVlZ8JCYmcvHiRVxdXbUupdjw9PTExcUl0+8uISGBgwcPyu/OSH///Tc3b94s1b8/RVF49913Wbt2LTt37sTT0zPT635+fpibm2f6vZ09e5aoqKhS+3t71jHLSkREBIBxv7U8NV8qZpYtW6ZYWloqixYtUv766y9lwIABioODgxITE6N1aUXS+++/r+zevVuJjIxU9u/frwQGBiqOjo5KXFyc1qUVKXfv3lWOHz+uHD9+XAGUGTNmKMePH1euXLmiKIqiTJkyRXFwcFDWr1+vnDhxQuncubPi6emppKSkaFy5tp523O7evauMHDlSCQ8PVyIjI5Vff/1VadSokVKjRg3l3r17WpeumUGDBin29vbK7t27lWvXrhmW5ORkwzYDBw5UqlatquzcuVM5cuSIEhAQoAQEBGhYtbaedcwuXLigfPrpp8qRI0eUyMhIZf369Uq1atWU559/3qjPKVVhqiiK8vXXXytVq1ZVLCwslKZNmyoHDhzQuqQiq3v37oqrq6tiYWGhVK5cWenevbty4cIFrcsqcnbt2qUATyy9e/dWFEXtHjN27FjF2dlZsbS0VF566SXl7Nmz2hZdBDztuCUnJytt27ZVKlasqJibmyvu7u5K//79S/0/fLM6XoCycOFCwzYpKSnK4MGDlXLlyik2NjZK165dlWvXrmlXtMaedcyioqKU559/XilfvrxiaWmpeHl5KaNGjVLi4+ON+hyZgk0IIYTIo1Jzz1QIIYQoKBKmQgghRB5JmAohhBB5JGEqhBBC5JGEqRBCCJFHEqZCCCFEHkmYCiGEEHkkYSqEEELkkYSpECLHdDod69at07oMIYocCVMhiok+ffqg0+meWNq1a6d1aUKUejKfqRDFSLt27Vi4cGGmdZaWlhpVI4R4SM5MhShGLC0tcXFxybSUK1cOUC/Bfvfdd7Rv3x5ra2uqVavGqlWrMu1/8uRJXnzxRaytralQoQIDBgwgMTEx0zYLFiygXr16WFpa4urqyrvvvpvp9Rs3btC1a1dsbGyoUaMGGzZsKNgvLUQxIGEqRAkyduxYXnnlFf744w969uzJ66+/zunTpwFISkoiKCiIcuXKcfjwYVauXMmvv/6aKSy/++47hgwZwoABAzh58iQbNmzAy8sr02dMnDiRbt26ceLECTp06EDPnj25detWoX5PIYqcfJ/vRghRIHr37q2Ympoqtra2mZb//ve/iqKoU00NHDgw0z7+/v7KoEGDFEVRlLlz5yrlypVTEhMTDa9v2rRJMTExMUxtVqlSJeXjjz/OtgZA+eSTTwzPExMTFUDZsmVLvn1PIYojuWcqRDHSunVrvvvuu0zrypcvb3gcEBCQ6bWAgAAiIiIAOH36ND4+Ptja2hpeb968OXq9nrNnz6LT6bh69SovvfTSU2to0KCB4bGtrS12dnbExcXl9isJUSJImApRjNja2j5x2TW/WFtb52g7c3PzTM91Oh16vb4gShKi2JB7pkKUIAcOHHjieZ06dQCoU6cOf/zxB0lJSYbX9+/fj4mJCbVq1aJs2bJ4eHgQFhZWqDULURLImakQxcj9+/eJiYnJtM7MzAxHR0cAVq5cSePGjWnRogU//fQThw4dYv78+QD07NmT8ePH07t3byZMmMD169d57733ePPNN3F2dgZgwoQJDBw4ECcnJ9q3b8/du3fZv38/7733XuF+USGKGQlTIYqRrVu34urqmmldrVq1OHPmDKC2tF22bBmDBw/G1dWVn3/+mbp16wJgY2PDtm3bGDZsGE2aNMHGxoZXXnmFGTNmGN6rd+/e3Lt3j5kzZzJy5EgcHR159dVXC+8LClFM6RRFUbQuQgiRdzqdjrVr19KlSxetSxGi1JF7pkIIIUQeSZgKIYQQeST3TIUoIeSOjRDakTNTIYQQIo8kTIUQQog8kjAVQggh8kjCVAghhMgjCVMhhBAijyRMhRBCiDySMBVCCCHySMJUCCGEyKP/B6aFJXxCE6imAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for e in range(0, len(epochs_list)):\n",
    "\n",
    "    epochs = list(range(1, epochs_list[e]+2))\n",
    "\n",
    "    # Normalize losses for visualization\n",
    "    max_train_loss = max(train_loss[e])\n",
    "    training_loss_normalized = [loss / max_train_loss for loss in train_loss[e]]\n",
    "    max_val_loss = max(val_loss[e])\n",
    "    val_loss_normalized = [loss / max_val_loss for loss in val_loss[e]]\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(epochs, training_loss_normalized, label='train loss', color='blue')\n",
    "    plt.plot(epochs, val_loss_normalized, label='val loss', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xticks(np.linspace(0, 25, 6, endpoint=True))\n",
    "    plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    # Format x-axis tick labels to display as integers\n",
    "    from matplotlib.ticker import FormatStrFormatter\n",
    "    plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "    plt.ylabel('Normalized Loss')\n",
    "    #plt.ylim(0.9, 1.0)\n",
    "    #plt.title(f'Search: {e+1} LR: {learning_rate[e]} BS: {batch_size[e]}')\n",
    "    plt.title(model_name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results of epoch with smallest test loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search 1 | Total epochs: 18 | Best epoch: 8 with loss: 191.51887118816376 and accuracy of 80.65% | LR: 5.5e-06 | BS: 16\n",
      "Highest accuracy: 82.89999999999999% at epoch 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Smallest test loss\n",
    "\n",
    "index_list = []\n",
    "\n",
    "for v in range(len(val_loss)):\n",
    "    lowest = min(val_loss[v])\n",
    "    index_lowest = val_loss[v].index(lowest)\n",
    "    acc = accuracies[v][index_lowest]\n",
    "    index_list.append(index_lowest)\n",
    "    acc_max = accuracies[v].index(max(accuracies[v]))\n",
    "    print(f\"Search {v+1} | Total epochs: {epochs_list[v]+1} | Best epoch: {index_lowest + 1} with loss: {lowest} and accuracy of {round(acc,4)*100}% | LR: {learning_rate[v]} | BS: {batch_size[v]}\")\n",
    "    print(f\"Highest accuracy: {round(max(accuracies[v]),4)*100}% at epoch {acc_max + 1}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Classification reports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification reports\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    target_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    target_names = [\n",
    "        \"antonymity\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonymity\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "classification_reports = []\n",
    "\n",
    "filename= \"cr_reports.txt\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    for e in range(len(epochs_list)):\n",
    "        for epoch in range(epochs_list[e]):\n",
    "            cr = classification_report(true_classes[e][epoch], \n",
    "                                val_preds[e][epoch],\n",
    "                                target_names=target_names, \n",
    "                                digits=4)\n",
    "            classification_reports.append(cr)\n",
    "\n",
    "            file.write(f\"Report of Search {e+1} for Epoch {epoch + 1}:\\n\\n{cr}\\n\")\n",
    "            file.write(\"-----End of Report-----\\n\\n\")\n",
    "        file.write(\"-----End of Search-----\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification report of epoch with lowest loss\n",
    "\n",
    "filename= \"cr_reports.txt\"\n",
    "\n",
    "with open(filename, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "searches = content.split(\"-----End of Search-----\\n\\n\")\n",
    "\n",
    "searches = [search for search in searches if search]\n",
    "\n",
    "reports = []\n",
    "\n",
    "for search in searches:\n",
    "    report = search.split(\"-----End of Report-----\\n\\n\")\n",
    "    reports.append(report)\n",
    "\n",
    "reports = [report for report in reports if report]\n",
    "\n",
    "i = 0\n",
    "step = 0\n",
    "\n",
    "for report in reports:\n",
    "    if step != 0:\n",
    "        i += 1\n",
    "    for r in report:\n",
    "        if report.index(r) == index_list[i]:\n",
    "            print(r)\n",
    "            step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    target_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    target_names = [\n",
    "        \"antonymity\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonymity\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "cm = confusion_matrix(true_classes[0][index_lowest], val_preds[0][index_lowest])\n",
    "\n",
    "# Plotting the confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(f'Confusion Matrix Val')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test_accuracies[0][12]\n",
    "print(f\"accuracy of {round(acc,4)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smallest test loss\n",
    "\n",
    "index_list = []\n",
    "\n",
    "for v in range(len(val_loss)):\n",
    "    #lowest = min(val_loss[v])\n",
    "    #index_lowest = test_loss[v][13]\n",
    "    acc = test_accuracies[v][12]\n",
    "    print(f\"accuracy of {round(acc,4)*100}%\")\n",
    "    #index_list.append(index_lowest)\n",
    "    #acc_max = test_accuracies[v].index(max(test_accuracies[v]))\n",
    "    #print(f\"Search {v+1} | Total epochs: {epochs_list[v]+1} | Best epoch: {index_lowest + 1} with loss: {lowest} and accuracy of {round(acc,4)*100}% | LR: {learning_rate[v]} | BS: {batch_size[v]}\")\n",
    "    #print(f\"Highest accuracy: {round(max(test_accuracies[v]),4)*100}% at epoch {acc_max + 1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification reports\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    target_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    target_names = [\n",
    "        \"antonymity\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonymity\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "classification_reports = []\n",
    "\n",
    "filename= \"test_reports.txt\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    for e in range(len(epochs_list)):\n",
    "        for epoch in range(epochs_list[e]):\n",
    "            cr = classification_report(test_true_classes[e][epoch], \n",
    "                                test_preds[e][epoch],\n",
    "                                target_names=target_names, \n",
    "                                digits=4)\n",
    "            classification_reports.append(cr)\n",
    "\n",
    "            file.write(f\"Report of Search {e+1} for Epoch {epoch + 1}:\\n\\n{cr}\\n\")\n",
    "            file.write(\"-----End of Report-----\\n\\n\")\n",
    "        file.write(\"-----End of Search-----\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    target_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    target_names = [\n",
    "        \"antonymity\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonymity\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "\n",
    "cm = confusion_matrix(test_true_classes[0][12], test_preds[0][12])\n",
    "\n",
    "# Plotting the confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(f'Confusion Matrix Test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Appendix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Test contradiction pre-trained models directly without fine-tuning on NLI dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli', use_fast=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained('symanto/xlm-roberta-base-snli-mnli-anli-xnli')\n",
    "\n",
    "#model_name = 'joeddav/xlm-roberta-large-xnli'\n",
    "model_name = 'symanto/xlm-roberta-base-snli-mnli-anli-xnli'\n",
    "\n",
    "# Function for encoding input data\n",
    "def encode_sets(data, tokenizer):\n",
    "    kwargs = { 'truncation': True,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'padding': 'max_length',\n",
    "    'return_attention_mask': True, \n",
    "    'return_token_type_ids': True     \n",
    "    }\n",
    "    datalist = list(zip(data['premise'], data['hypothesis']))\n",
    "    tokenized = tokenizer.batch_encode_plus(datalist,**kwargs)\n",
    "    input_ids = torch.LongTensor(tokenized.input_ids)\n",
    "    attention_masks = torch.LongTensor(tokenized.attention_mask)\n",
    "    token_type_ids = torch.LongTensor(tokenized.token_type_ids)\n",
    "    return input_ids, attention_masks, token_type_ids\n",
    "\n",
    "# Get number of labels in dataset\n",
    "num_classes = dataset['label'].nunique()\n",
    "\n",
    "# label names according to 3 label or 11 label dataset\n",
    "if num_classes == 3:\n",
    "    label_names = [\n",
    "    \"entailment\",\n",
    "    \"neutral\",\n",
    "    \"contradiction\"\n",
    "] \n",
    "elif num_classes == 11:\n",
    "    label_names = [\n",
    "        \"antonymity\",\n",
    "        \"entailment\",\n",
    "        \"factive_antonymity\",\n",
    "        \"factive_embedding_verb\",\n",
    "        \"lexical\",\n",
    "        \"negation\",\n",
    "        \"neutral\",\n",
    "        \"numeric\",\n",
    "        \"structure\",\n",
    "        \"temporal\",\n",
    "        \"worldknowledge\"\n",
    "    ]\n",
    "    \n",
    "# Set cross-entropy loss as loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Testing:\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# Encode test set\n",
    "input_ids, attention_masks, token_type_ids = encode_sets(test,tokenizer)\n",
    "labels = torch.Tensor(test['label']).reshape(-1, 1)\n",
    "test_tensor = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
    "test_dataloader = DataLoader(test_tensor, sampler=SequentialSampler(test_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "\n",
    "for j, batch in enumerate(test_dataloader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_ids, attention_masks, token_type_ids = (batch[0].to(device), \n",
    "                                                        batch[1].to(device),\n",
    "                                                        batch[2].to(device))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "        end_time = time.time()\n",
    "\n",
    "    logits = outputs.logits\n",
    "    # Predicted labels\n",
    "    labels = torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device)\n",
    "    loss = criterion(logits, labels)\n",
    "    # True labels\n",
    "    test_true = batch[3]\n",
    "\n",
    "    test_preds = logits.detach().cpu().numpy()\n",
    "    test_loss += loss.item()\n",
    "\n",
    "    if j == 0:  # first batch\n",
    "        stacked_test_preds = test_preds\n",
    "        true_labels = test_true\n",
    "    else:\n",
    "        stacked_test_preds = np.vstack((stacked_test_preds, test_preds))\n",
    "        true_labels = np.vstack((true_labels, test_true))\n",
    "\n",
    "test_preds = np.argmax(stacked_test_preds, axis=1)\n",
    "test_true_classes = true_labels.flatten().astype('int64')\n",
    "test_accuracy = np.sum(test_preds == test_true_classes)/len(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = np.sum(test_preds == test_true_classes)/len(test_preds)\n",
    "print(f\"Accuracy on test set: {round(test_accuracy,3)*100}% with loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Unfreezing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_layers(model, last_n_layers):\n",
    "    # Reverse the model layers and unfreeze the last n layers\n",
    "    layers_to_unfreeze = list(model.roberta.encoder.layer.children())[-last_n_layers:]\n",
    "    for layer in layers_to_unfreeze:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "test_preds = []\n",
    "true_classes = []\n",
    "accuracies = []\n",
    "\n",
    "phases = [2, 4, 6, 8, 10, 12]\n",
    "for index, phase in enumerate(phases):\n",
    "    unfreeze_layers(model, phase)\n",
    "    print(f\"Training with last {phase} layers unfrozen...\")\n",
    "\n",
    "    train_loss_phase = []\n",
    "    test_loss_phase = []\n",
    "\n",
    "    test_preds_phase = []\n",
    "    true_classes_phase = []\n",
    "    accuracies_phase = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        total_train_loss=0\n",
    "            \n",
    "        for i,batch in tqdm(enumerate(train_dataloader)):\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids, attention_masks, token_type_ids, labels=(batch[0].to(device), \n",
    "                                                                batch[1].to(device),\n",
    "                                                                batch[2].to(device),  \n",
    "                                                                torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device))\n",
    "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks, labels=labels)\n",
    "            loss=outputs[0]\n",
    "            #if i%10==0:\n",
    "                #print(f'loss of batch {i}: {loss}')\n",
    "            total_train_loss+=loss.item()\n",
    "\n",
    "            # Backpropagation:\n",
    "            # calculate gradients\n",
    "            loss.backward()\n",
    "            # clip the norm of the gradients to 1.0. (prevents gradient explosion)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # parameter update\n",
    "            optimizer.step()\n",
    "\n",
    "            # Scheduler update\n",
    "            #scheduler.step()\n",
    "        \n",
    "        print(f'Training loss on epoch {epoch + 1}: {total_train_loss}')\n",
    "        train_loss_phase.append(total_train_loss)\n",
    "\n",
    "        # Testing\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        torch.set_grad_enabled(False)\n",
    "        total_test_loss = 0\n",
    "\n",
    "        for j, batch in enumerate(val_dataloader):\n",
    "            input_ids, attention_masks, token_type_ids, labels=(batch[0].to(device), \n",
    "                                                                batch[1].to(device),\n",
    "                                                                batch[2].to(device), \n",
    "                                                                torch.nn.functional.one_hot(batch[3].to(torch.int64), num_classes=num_classes).squeeze(1).float().to(device))\n",
    "            outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks, labels = labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            logits = outputs.logits\n",
    "            true = batch[3]\n",
    "\n",
    "            val_preds = logits.detach().cpu().numpy()\n",
    "            total_test_loss += loss.item()\n",
    "            if j == 0:  # first batch\n",
    "                stacked_val_preds = val_preds\n",
    "                true_labels = true\n",
    "\n",
    "            else:\n",
    "                stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n",
    "                true_labels = np.vstack((true_labels, true))\n",
    "                #stacked_val_preds.extend(val_preds)\n",
    "            #print(len(stacked_val_preds))\n",
    "        \n",
    "        print(f'\\n------------------------------------\\nTest loss on epoch {epoch + 1}: {total_test_loss}\\n------------------------------------\\n')\n",
    "\n",
    "        # Prediction results per epoch\n",
    "\n",
    "        test_preds_phase.append(np.argmax(stacked_val_preds, axis=1))\n",
    "        true_classes_phase.append(true_labels.flatten().astype('int64'))\n",
    "        accuracies_phase.append(np.sum(test_preds_phase[epoch] == true_classes_phase[epoch])/len(test_preds_phase[epoch]))\n",
    "\n",
    "        # Test loss per epoch\n",
    "\n",
    "        test_loss_phase.append(total_test_loss)\n",
    "    \n",
    "    train_loss.append(train_loss_phase)\n",
    "    test_loss.append(test_loss_phase)\n",
    "\n",
    "    test_preds.append(test_preds_phase)\n",
    "    true_classes.append(true_classes_phase)\n",
    "    accuracies.append(accuracies_phase)\n",
    "\n",
    "    NUM_EPOCHS = NUM_EPOCHS + 5\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\n",
    "    \"antonymity\",\n",
    "    \"entailment\",\n",
    "    \"factive_antonymity\",\n",
    "    \"factive_embedding_verb\",\n",
    "    \"lexical\",\n",
    "    \"negation\",\n",
    "    \"neutral\",\n",
    "    \"numeric\",\n",
    "    \"structure\",\n",
    "    \"temporal\",\n",
    "    \"worldknowledge\"\n",
    "]   \n",
    "\n",
    "classification_reports = []\n",
    "\n",
    "filename= \"test.txt\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    for epoch in range(stop_epochs-1):\n",
    "        cr = classification_report(true_classes[epoch], \n",
    "                            val_preds[epoch],\n",
    "                            target_names=target_names, \n",
    "                            digits=4)\n",
    "        classification_reports.append(cr)\n",
    "\n",
    "        file.write(f\"Report for Epoch {epoch + 1}:\\n{cr}\\n\")\n",
    "        file.write(\"-----End of Report-----\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= \"test.txt\"\n",
    "\n",
    "with open(filename, \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Split the content back into individual reports using the delimiter\n",
    "# This creates a list where each element is a report\n",
    "reports = content.split(\"-----End of Report-----\\n\\n\")\n",
    "\n",
    "# Remove any empty strings in case they exist\n",
    "reports = [report for report in reports if report]\n",
    "\n",
    "# Iterate over the reports and print them\n",
    "for report in reports[index_lowest:index_lowest+1]:\n",
    "    print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
