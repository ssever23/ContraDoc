{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from PyPDF2 import PdfReader\n",
    "import fitz\n",
    "\n",
    "# Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Store\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# additional libraries\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download punkt tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vector DB with Langchain chunking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load and split PDFs with Langchain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 400, chunk_overlap = 100, add_start_index = False) # splits the text into chunks\n",
    "\n",
    "def load_pdfs_langchain(directory_path):\n",
    "    chunks = []\n",
    "    if not os.path.exists(directory_path):\n",
    "        logging.error(f\"Directory path does not exist: {directory_path}\")\n",
    "        return chunks\n",
    "    \n",
    "    for fn in os.listdir(directory_path):\n",
    "        if fn.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(directory_path, fn)\n",
    "            try:\n",
    "                loader = PyPDFLoader(filepath).load()\n",
    "                split_text = text_splitter.split_documents(loader)\n",
    "                chunks.extend(split_text) \n",
    "                logging.info(f\"Processed {fn} with {len(split_text)} chunks.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to process {fn}: {e}\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/home/ssever/ContraDoc/data/PDFs\"\n",
    "chunks = load_pdfs_langchain(directory_path=directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Embedding and storing in vector database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the embedding function\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L12-v2\")\n",
    "\n",
    "# Load pdfs into vector database\n",
    "destination = \"./vector_store/kb_langchain\"\n",
    "db = Chroma.from_documents(chunks, embedding_function, persist_directory=destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vector DB with custom chunking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load and split PDFs with custom chunking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_by_sentences(text, max_chunk_length=400):\n",
    "    # Tokenize the document into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Initialize variables to store chunks and the current chunk content\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    last_sentence = \"\"\n",
    "\n",
    "    # Loop through each sentence, grouping them into chunks\n",
    "    for sentence in sentences:\n",
    "        # If the current chunk plus the new sentence is too long, start a new chunk\n",
    "        if len(current_chunk) + len(sentence) > max_chunk_length:         \n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            # Start new chunk with the last sentence of the previous chunk for overlap\n",
    "            current_chunk = last_sentence + \" \" + sentence\n",
    "        # Otherwise, add the sentence to the current chunk\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                current_chunk += \" \" + sentence\n",
    "            else:\n",
    "                current_chunk = sentence\n",
    "        # Update last_sentence to the current one\n",
    "        last_sentence = sentence\n",
    "\n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_custom_chunks(directory_path):\n",
    "    all_chunks = []\n",
    "    if not os.path.exists(directory_path):\n",
    "        logging.error(f\"Directory path does not exist: {directory_path}\")\n",
    "        return chunks\n",
    "    \n",
    "    for fn in os.listdir(directory_path):\n",
    "        if fn.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(directory_path, fn)\n",
    "            try:\n",
    "                loader = PyPDFLoader(filepath).load()\n",
    "                page_content = [loader[k].page_content for k in range(0, len(loader))]\n",
    "                page_content = ' '.join(page_content)\n",
    "                #metadata = [loader[k].metadata for k in range(0, len(loader))]\n",
    "                chunks = chunk_text_by_sentences(page_content)\n",
    "                all_chunks.extend(chunks)\n",
    "                logging.info(f\"Processed {fn} with {len(all_chunks)} chunks.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to process {fn}: {e}\")\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/home/ssever/ContraDoc/data/PDFs\"\n",
    "chunked_sentences = load_pdfs_custom_chunks(directory_path=directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document class to extract page content\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata=None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Content: {self.page_content}, Metadata: {self.metadata}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_content = [Document(page_content=chunk) for chunk in chunked_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Embedding and storing into vector database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the embedding function\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L12-v2\")\n",
    "\n",
    "# Load pdfs into vector database\n",
    "destination = \"./vector_store/kb_custom_chunks\"\n",
    "db = Chroma.from_documents(chunk_content, embedding_function, persist_directory=destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vector DB with custom sentences chunking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load and split PDFs with sentence tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_custom_sentences(directory_path):\n",
    "    all_sentences = []\n",
    "    if not os.path.exists(directory_path):\n",
    "        logging.error(f\"Directory path does not exist: {directory_path}\")\n",
    "        return chunks\n",
    "    \n",
    "    for fn in os.listdir(directory_path):\n",
    "        if fn.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(directory_path, fn)\n",
    "            try:\n",
    "                loader = PyPDFLoader(filepath).load()\n",
    "                page_content = [loader[k].page_content for k in range(0, len(loader))]\n",
    "                page_content = ' '.join(page_content)\n",
    "                #metadata = [loader[k].metadata for k in range(0, len(loader))]\n",
    "                sentences = sent_tokenize(page_content)\n",
    "                all_sentences.extend(sentences)\n",
    "                logging.info(f\"Processed {fn} with {len(all_sentences)} chunks.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to process {fn}: {e}\")\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/home/ssever/ContraDoc/data/PDFs\"\n",
    "sentences = load_pdfs_custom_sentences(directory_path=directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_content = [Document(page_content=sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Embedding and storing into vector database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the embedding function\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L12-v2\")\n",
    "\n",
    "# Load pdfs into vector database\n",
    "destination = \"./vector_store/kb_custom_sentences\"\n",
    "db = Chroma.from_documents(sentences_content, embedding_function, persist_directory=destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Appendix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **PDF highlighting test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fitz.utils import getColorList\n",
    "cl = getColorList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_document = fitz.open(\"/home/user123/ContraDoc/data/PDFs/Transkript.pdf\")\n",
    "page = pdf_document.load_page(1)\n",
    "matches = page.search_for(\"Before we go further, I would really like to thank and recognize our employees, our customers, our business partners all around\\\n",
    "                          the world for supporting our business in these extraordinary times.\")\n",
    "\n",
    "for match in matches:\n",
    "            highlight = page.add_highlight_annot(match)\n",
    "            highlight.set_colors(stroke=fitz.pdfcolor[\"skyblue\"])\n",
    "            highlight.update()\n",
    "\n",
    "output_pdf_path = os.path.splitext(\"/home/user123/ContraDoc/data/PDFs/Transkript.pdf\")[0] + \"_highlighted.pdf\"\n",
    "pdf_document.save(output_pdf_path)\n",
    "pdf_document.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Order intake grew 31% in EMEA, 54% in Americas, 15% in  APAC,  and  globally,  orders  were  up  29%,  reaching  87,000 units, and 87% of those units were electrified.\"\n",
    "docs = db.similarity_search_with_score(query, k=5)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Backup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"/home/user123/ContraDoc/data/PDFs/Transkript.pdf\")\n",
    "\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# load it into Chroma\n",
    "db2 = Chroma.from_documents(pages, embedding_function)\n",
    "\n",
    "# query it\n",
    "query = \"Order intake grew 31% in EMEA, 54% in Americas, 15% in  APAC,  and  globally,  orders  were  up  29%,  reaching  87,000 units, and 87% of those units were electrified.\"\n",
    "docs = db2.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 400, chunk_overlap = 50, add_start_index = False) # splits the text into chunks\n",
    "\n",
    "loader = PyPDFLoader(\"/home/user123/ContraDoc/data/PDFs/Transkript.pdf\").load()\n",
    "text = text_splitter.split_documents(loader)\n",
    "\n",
    "db = Chroma.from_documents(text, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Order intake grew 31% in EMEA, 54% in Americas, 15% in  APAC,  and  globally,  orders  were  up  29%,  reaching  87,000 units, and 87% of those units were electrified.\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
