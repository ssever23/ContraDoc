{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting sentences\n",
    "import fitz  # PyMuPDF\n",
    "import nltk\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# pair generation\n",
    "import openai\n",
    "import json\n",
    "from category_types import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extract sentences from documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download punkt tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break content of pdf files into individual sentences\n",
    "folder_path = \"/home/ssever/ContraDoc/data/PDFs\"\n",
    "\n",
    "pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\n",
    "\n",
    "all_sentences = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    full_text = \"\"\n",
    "    doc = fitz.open(pdf_file)\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    \n",
    "    doc.close()\n",
    "\n",
    "    sentences = nltk.tokenize.sent_tokenize(full_text)\n",
    "\n",
    "    all_sentences.append(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all newline characters\n",
    "\n",
    "flattened_and_cleaned_list = [item.replace(\"\\n\", \"\") for sublist in all_sentences for item in sublist]\n",
    "print(f\"Number of sentences:\", len(flattened_and_cleaned_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all sentences in text file\n",
    "filepath = \"/home/ssever/ContraDoc/data/text_files/all_sentences\"\n",
    "\n",
    "# Open the file in write mode ('w') and write each sentence to the file\n",
    "with open(filepath, 'w') as file:\n",
    "    for sentence in flattened_and_cleaned_list:\n",
    "        file.write(sentence + '\\n')  # Add '\\n' to ensure each sentence is on a new line\n",
    "\n",
    "print(f\"Sentences have been written to {filepath}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the file in read mode \n",
    "my_file = open(\"/home/ssever/ContraDoc/data/text_files/all_sentences\", \"r\") \n",
    "\n",
    "# reading the file \n",
    "all_sentences = my_file.read()\n",
    "all_sentences = all_sentences.split('\\n')\n",
    "\n",
    "my_file.close()\n",
    "\n",
    "len(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all sentences in token length between 10 and 45\n",
    "\n",
    "filtered_sentences = [sentence for sentence in all_sentences if len(nltk.word_tokenize(sentence)) <= 45 and len(nltk.word_tokenize(sentence)) >= 10]\n",
    "print(f\"Number of filtered sentences:\", len(filtered_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 3000 random sentences for pair generation\n",
    "random_sentences = random.sample(filtered_sentences, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sentence with the least amount of tokens\n",
    "\n",
    "min_tokens = float('inf')\n",
    "sentence_with_least_tokens = \"\"\n",
    "\n",
    "for sentence in filtered_sentences:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    if len(tokens) < min_tokens:\n",
    "        min_tokens = len(tokens)\n",
    "        sentence_with_least_tokens = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sentence with the most tokens: \\\"{sentence_with_least_tokens}\\\"\")\n",
    "print(f\"Number of tokens: {min_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create pairs for training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category types used for pair generation\n",
    "category_types = [antonym, negation, numeric, factive_embedded_verb, factive_antonym, structure, lexical, temporal, wk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose sentences for category pair generation\n",
    "train_premises = random_sentences[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPT api\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "openai.api_key = (api_key)\n",
    "model= 'gpt-4-turbo-preview'\n",
    "max_tokens = 1024\n",
    "temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main cell: Contains prompt for pair generation and thus generates all pairs for the NLI dataset\"\"\"\n",
    "\n",
    "all_responses=[]\n",
    "\n",
    "num_index = len(category_types)\n",
    "index = 0\n",
    "\n",
    "for premise in train_premises:\n",
    "    if index == num_index:\n",
    "            index = 0\n",
    "    response=[]\n",
    "    res = openai.ChatCompletion.create(\n",
    "              model=model,\n",
    "              max_tokens=max_tokens,\n",
    "              temperature = temperature,\n",
    "              messages=[{\"role\": \"system\", \"content\": \"You are an expert on semantics and linguistics, with a profound knowledge\\\n",
    "              in Natural Language Processing. You are especially aware of the work by Marneffe et al., classifying\\\n",
    "              different types of contradictions, such as antonyms, negations, numerical mismatches, factive, structural, lexical, and world knowledge contradictions. To this end,\\\n",
    "              a contradiction is defined as a mismatch between two statements, such that they cannot possibly both be true.\\\n",
    "              It is assumed, that both statements refer to the same fact or event, even if this is not explicitly stated. The premise is provided,\\\n",
    "              you have to create a hypothesis of one of the contradiction types for this premise.\"},\n",
    "              {\"role\": \"user\", \"content\": f\"Please generate one contradictory hypothesis for a {premise}, based on {category_types[index].description}. The contradictions\\\n",
    "              hould be original and reasonably different from each other.\\\n",
    "              Format your response in the following way: {category_types[index].name} P: [PREMISE]. H: [HYPOTHESIS]. Make sure to include {category_types[index].name}\"},\n",
    "              {\"role\": \"assistant\", \"content\": category_types[index].description}],\n",
    "            )\n",
    "\n",
    "    response.append(res[\"choices\"][0][\"message\"][\"content\"])\n",
    "    index += 1\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contradiction prompt:\n",
    "\n",
    "[{\"role\": \"system\", \"content\": \"You are an expert on semantics and linguistics, with a profound knowledge\\\n",
    "in Natural Language Processing. You are especially aware of the work by Marneffe et al., classifying\\\n",
    "different types of contradictions, such as antonyms, negations, numerical mismatches, factive, structural, lexical, and world knowledge contradictions. To this end,\\\n",
    "a contradiction is defined as a mismatch between two statements, such that they cannot possibly both be true.\\\n",
    "It is assumed, that both statements refer to the same fact or event, even if this is not explicitly stated.The premise is provided,\\\n",
    "you have to create a hypothesis of one of the contradiction types for this premise.\"},\n",
    "{\"role\": \"user\", \"content\": f\"Please generate one contradictory hypothesis for a {premise}, based on {category_types[index].description}. The contradictions\\\n",
    "should be original and reasonably different from each other.\\\n",
    "Format your response in the following way: {category_types[index].name} P: [PREMISE]. H: [HYPOTHESIS]. Make sure to include {category_types[index].name}\"},\n",
    "{\"role\": \"assistant\", \"content\": category_types[index].description}]\n",
    "\n",
    "# entailment, neutral prompt:\n",
    "\n",
    "[{\"role\": \"system\", \"content\": \"You are an expert on semantics and linguistics, with a profound knowledge\\\n",
    "in Natural Language Processing. You are aware of the work of classifying entailments and neutral pairs of statements. To this end,\\\n",
    "an entailment is defined in that two statements are entailed if the truth of the second statement follows from the truth of the first statement.\\\n",
    "Statements of neutral pairs do neither entail nor contradict each other.\\\n",
    "In the case of entailment it is assumed, that both statements refer to the same fact or event, even if this is not explicitly stated.\\\n",
    "The Premise is provided, you have to create a hypothesis for this premise.\"},\n",
    "{\"role\": \"user\", \"content\": f\"Please generate one hypothesis for a {premise}, based on {category_types[index].description}. The hypotheses\\\n",
    "should be original and reasonably different from each other.\\\n",
    "Format your response in the following way: {category_types[index].name} P: [PREMISE]. H: [HYPOTHESIS]. Make sure to include {category_types[index].name}\"},\n",
    "{\"role\": \"assistant\", \"content\": category_types[index].description}]\n",
    "\n",
    "# numeric mimsatch prompt:\n",
    "\n",
    "[{\"role\": \"system\", \"content\": \"You are an expert on semantics and linguistics, with a profound knowledge\\\n",
    " in Natural Language Processing. You are especially aware of contradictions, such as numerical mismatches. To this end, A contradiction based on a numerical mismatch means that a contradiction arises\\\n",
    " between two statements (Premise and Hypothesis) because there are mismatching numbers in premise and hypothesis. The contradiction only changes the numerical values. Don't change anything else in the text.\\\n",
    " The premise is provided, you have to create a hypothesis for a numerical mismatch for this premise.\"},\n",
    " {\"role\": \"user\", \"content\": f\"Please generate numerical mismatch hypothesis for a {premise}, based on {category_types[index].description}.\\\n",
    " Format your response in the following way: {category_types[index].name} P: [PREMISE]. H: [HYPOTHESIS]. Make sure to include {category_types[index].name}\"},\n",
    " {\"role\": \"assistant\", \"content\": category_types[index].description}]\n",
    "\n",
    "# structure prompt:\n",
    "\n",
    "[{\"role\": \"system\", \"content\": \"You are an expert on semantics and linguistics, with a profound knowledge\\\n",
    " in Natural Language Processing. You are especially aware of structural contradictions. To this end, A structural contradiction means that a contradiction arises\\\n",
    " between two statements (Premise and Hypothesis) because there is a mismatch in the sentence structure. The contradiction only changes the sentence structure.\\\n",
    " Don't change or add anything to the verb of the sentence. Don't change the subject of the phrase. Only change the object or the subject of the phrase.\\\n",
    " The premise is provided, you have to create a hypothesis for a structural mismatch for this premise.\"},\n",
    " {\"role\": \"user\", \"content\": f\"Please generate a structural mismatch hypothesis for a {premise}, based on {category_types[index].description}. An example of a structural mismatch is given in {category_types[index].instances}\\\n",
    " Format your response in the following way: {category_types[index].name} P: [PREMISE]. H: [HYPOTHESIS]. Make sure to include {category_types[index].name}\"},\n",
    " {\"role\": \"assistant\", \"content\": category_types[index].description}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Store pairs in csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Split sentence pairs into premise, hypothesis and label\n",
    "\n",
    "split_data = []\n",
    "for item in response:\n",
    "    for i in item:\n",
    "        if not i.startswith('P'):\n",
    "            parts = i.split(' P: ')\n",
    "            label = parts[0]\n",
    "            premise_hypothesis = parts[1].split(' H: ')\n",
    "            premise = premise_hypothesis[0]\n",
    "            hypothesis = premise_hypothesis[1]\n",
    "            split_data.append([premise, hypothesis, label])\n",
    "\n",
    "with open('/home/ssever/ContraDoc/data/csv_files/gpt_contradictions.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"premise\", \"hypothesis\", \"label\"])\n",
    "    writer.writerows(split_data[:167])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Combine contradictions CSV file with entailment and neutral CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files\n",
    "file1_path = '/home/ssever/ContraDoc/data/csv_files/gpt_contradictions.csv'\n",
    "file2_path = '/home/ssever/ContraDoc/data/csv_files/gpt_entail_neutral.csv'\n",
    "\n",
    "df1 = pd.read_csv(file1_path)\n",
    "df2 = pd.read_csv(file2_path)\n",
    "\n",
    "# Append the rows of the second dataframe to the first dataframe\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_csv_path = '/home/ssever/ContraDoc/data/csv_files/combined_data_set.csv'\n",
    "combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f'Combined CSV saved to {combined_csv_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Clean and transform csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unique IDs to dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Function to generate a unique alphanumeric id\n",
    "def generate_unique_id(length=10):\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "# CSV is loaded into a DataFrame\n",
    "df = pd.read_csv('/home/ssever/ContraDoc/data/csv_files/combined_data_set.csv')\n",
    "\n",
    "# Generate uniqze ids\n",
    "unique_ids = set()\n",
    "while len(unique_ids) < len(df):\n",
    "    unique_ids.add(generate_unique_id())\n",
    "\n",
    "# Insert ids into table\n",
    "df.insert(0, 'id', list(unique_ids))\n",
    "\n",
    "# make all labels lowercase\n",
    "df['label'] = df['label'].str.lower()\n",
    "\n",
    "# align elements to the left\n",
    "styled_df = df.style.set_properties(**{'text-align': 'left'})\n",
    "styled_df.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "\n",
    "# Insert into CSV file\n",
    "df.to_csv('/home/ssever/ContraDoc/data/csv_files/combined_data_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change label string values to numeric values\n",
    "\n",
    "df = pd.read_csv('/home/ssever/ContraDoc/data/csv_files/combined_data_set.csv')\n",
    "\n",
    "# Strip leading and trailing spaces from the 'label' column\n",
    "df['label'] = df['label'].str.strip()\n",
    "\n",
    "# Identify unique words and sort them to maintain consistency\n",
    "unique_words = sorted(df['label'].unique())\n",
    "\n",
    "# Create a mapping from words to digits\n",
    "word_to_digit = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "# Apply the mapping to the 'label' column\n",
    "df['label_digit'] = df['label'].map(word_to_digit)\n",
    "\n",
    "# Rename the original 'label' column to 'label_string'\n",
    "df.rename(columns={'label': 'label_string', 'label_digit': 'label'}, inplace=True)\n",
    "\n",
    "# Adjusting the column order, ensuring 'label_string' is positioned next to 'label'\n",
    "columns_order = ['id', 'premise', 'hypothesis', 'label', 'label_string'] + [col for col in df.columns if col not in ['id', 'premise', 'hypothesis', 'label', 'label_string']]\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV\n",
    "shuffled_df.to_csv('/home/ssever/ContraDoc/data/csv_files/nli_data_set.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
